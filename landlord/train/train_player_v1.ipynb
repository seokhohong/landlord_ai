{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.2.tar.gz (8.3 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.2-py3-none-any.whl size=10911 sha256=cac2bd30d51d2c6f92903a40c56e4652db225ac7d47653ed7edf4a3edf9c4d03\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/38/17/72/a99b2187dd9be6cc1ebf0fddc63d16740231db8acf0e3db197\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "Successfully installed landlord-ai-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai\n",
    "!pip install keras.preprocessing --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from landlordai.game.player import LearningPlayer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_ids, batch_size=1024, shuffle=True, clamp=False):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.path_ids = path_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.clamp = clamp\n",
    "        \n",
    "        self.load_cache()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return 1000\n",
    "\n",
    "    def load_cache(self):\n",
    "        with open(random.choice(self.path_ids), 'rb') as f:\n",
    "            self.cache = pickle.load(f)\n",
    "        self.curr_index = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        limit = min(len(self.cache[0]), (self.curr_index + 1) * self.batch_size)\n",
    "        \n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        history_matrices = self.cache[0][self.curr_index * self.batch_size: limit]\n",
    "        move_vectors = self.cache[1][self.curr_index * self.batch_size: limit]\n",
    "        hand_vectors = self.cache[2][self.curr_index * self.batch_size: limit]\n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        y = self.cache[3][self.curr_index * self.batch_size: limit]\n",
    "        self.curr_index += 1\n",
    "        \n",
    "        # load a new batch\n",
    "        if (self.curr_index + 1) * self.batch_size >= len(self.cache[0]):\n",
    "            self.load_cache()\n",
    "        \n",
    "        return [self.densify(history_matrices), move_vectors], self.adjust_y(y)\n",
    "\n",
    "    def densify(self, sparse_matrix):\n",
    "        return np.array([x.todense() for x in sparse_matrix])\n",
    "\n",
    "    def adjust_y(self, y):\n",
    "        if not self.clamp:\n",
    "            return y\n",
    "        new_y = []\n",
    "        for elem in y:\n",
    "            if abs(int(elem) - elem) > 1E-4:\n",
    "                new_y.append(0)\n",
    "            else:\n",
    "                new_y.append(elem)\n",
    "        return np.array(new_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '3_29_sim6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://landlord_ai/3_29_sim6/0.pkl...\n",
      "Copying gs://landlord_ai/3_29_sim6/1.pkl...\n",
      "| [2/2 files][299.4 MiB/299.4 MiB] 100% Done                                    \n",
      "Operation completed over 2 objects/299.4 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!rm -r ../data/$data_folder\n",
    "!gsutil -m cp -r gs://landlord_ai/$data_folder ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '../data/' + data_folder\n",
    "filenames = [directory + '/' + file for file in os.listdir(directory)]\n",
    "\n",
    "if len(filenames) == 1:\n",
    "    train_path_ids = filenames\n",
    "    test_path_ids = filenames\n",
    "else:\n",
    "    divider = int(len(filenames) * 0.9)\n",
    "    train_path_ids = filenames[divider:]\n",
    "    test_path_ids = filenames[:divider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp = True\n",
    "train_gen = DataGenerator(train_path_ids, clamp=clamp)\n",
    "test_gen = DataGenerator(test_path_ids, clamp=clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(train_gen[0][0][0], train_gen[0][0][0])\n",
    "for i in range(3):\n",
    "    get_set = train_gen[0][0][0]\n",
    "    if len(get_set.shape) != 3:\n",
    "        print(get_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_DIM = 64\n",
    "K.clear_session()\n",
    "\n",
    "history_inp = Input((LearningPlayer_v1.TIMESTEPS, LearningPlayer_v1.TIMESTEP_FEATURES), name='history_inp')\n",
    "move_inp = Input((LearningPlayer_v1.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "gru = GRU(GRU_DIM, name='gru')(history_inp)\n",
    "\n",
    "concat = Concatenate()([gru, move_inp])\n",
    "hidden = Dense(32, activation='relu', name='hidden')(concat)\n",
    "\n",
    "output = Dense(1, activation='linear', name='output')(hidden)\n",
    "combined_net = keras.models.Model(inputs=[history_inp, move_inp], outputs=output)\n",
    "combined_net.compile(loss=mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "history_inp (InputLayer)        (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 64)           16512       history_inp[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "move_inp (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 85)           0           gru[0][0]                        \n",
      "                                                                 move_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 32)           2752        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            33          hidden[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 19,297\n",
      "Trainable params: 19,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.5831 - mean_squared_error: 0.5831 - val_loss: 0.4308 - val_mean_squared_error: 0.4308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43084, saving model to combined0.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.5702 - mean_squared_error: 0.5702 - val_loss: 0.4318 - val_mean_squared_error: 0.4318\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.43084\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 0.4312 - val_mean_squared_error: 0.4312\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.43084\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.5670 - mean_squared_error: 0.5670 - val_loss: 0.4309 - val_mean_squared_error: 0.4309\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43084\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.5784 - mean_squared_error: 0.5784 - val_loss: 0.4315 - val_mean_squared_error: 0.4315\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43084\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.5811 - mean_squared_error: 0.5811 - val_loss: 0.4314 - val_mean_squared_error: 0.4314\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43084\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.5797 - mean_squared_error: 0.5797 - val_loss: 0.4328 - val_mean_squared_error: 0.4328\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43281, saving model to combined1.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.5577 - mean_squared_error: 0.5577 - val_loss: 0.4276 - val_mean_squared_error: 0.4276\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43281 to 0.42763, saving model to combined1.h5\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.5673 - mean_squared_error: 0.5673 - val_loss: 0.4137 - val_mean_squared_error: 0.4137\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42763 to 0.41368, saving model to combined1.h5\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.5576 - mean_squared_error: 0.5576 - val_loss: 0.4250 - val_mean_squared_error: 0.4250\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.41368\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.5625 - mean_squared_error: 0.5625 - val_loss: 0.4121 - val_mean_squared_error: 0.4121\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41368 to 0.41213, saving model to combined1.h5\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.5479 - mean_squared_error: 0.5479 - val_loss: 0.4194 - val_mean_squared_error: 0.4194\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.41213\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.5622 - mean_squared_error: 0.5622 - val_loss: 0.4170 - val_mean_squared_error: 0.4170\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41213\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.5518 - mean_squared_error: 0.5518 - val_loss: 0.4155 - val_mean_squared_error: 0.4155\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41213\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.5589 - mean_squared_error: 0.5589 - val_loss: 0.4207 - val_mean_squared_error: 0.4207\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41213\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.5426 - mean_squared_error: 0.5426 - val_loss: 0.4225 - val_mean_squared_error: 0.4225\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41213\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 0.4081 - val_mean_squared_error: 0.4081\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40807, saving model to combined2.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.5360 - mean_squared_error: 0.5360 - val_loss: 0.4902 - val_mean_squared_error: 0.4902\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.40807\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.5252 - mean_squared_error: 0.5252 - val_loss: 0.3932 - val_mean_squared_error: 0.3932\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40807 to 0.39321, saving model to combined2.h5\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.5223 - mean_squared_error: 0.5223 - val_loss: 0.3907 - val_mean_squared_error: 0.3907\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39321 to 0.39067, saving model to combined2.h5\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.5062 - mean_squared_error: 0.5062 - val_loss: 0.4003 - val_mean_squared_error: 0.4003\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.39067\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.5054 - mean_squared_error: 0.5054 - val_loss: 0.4210 - val_mean_squared_error: 0.4210\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39067\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.4889 - mean_squared_error: 0.4889 - val_loss: 0.5819 - val_mean_squared_error: 0.5819\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39067\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.5249 - mean_squared_error: 0.5249 - val_loss: 0.4583 - val_mean_squared_error: 0.4583\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39067\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.4886 - mean_squared_error: 0.4886 - val_loss: 0.5008 - val_mean_squared_error: 0.5008\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39067\n",
      "Epoch 00009: early stopping\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.4693 - mean_squared_error: 0.4678 - val_loss: 0.5883 - val_mean_squared_error: 0.5883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58834, saving model to combined3.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.4762 - mean_squared_error: 0.4762 - val_loss: 0.3926 - val_mean_squared_error: 0.3926\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58834 to 0.39256, saving model to combined3.h5\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.4793 - mean_squared_error: 0.4793 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39256 to 0.35701, saving model to combined3.h5\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.4744 - mean_squared_error: 0.4744 - val_loss: 0.5511 - val_mean_squared_error: 0.5511\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35701\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.4910 - mean_squared_error: 0.4910 - val_loss: 0.5117 - val_mean_squared_error: 0.5117\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35701\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.4514 - mean_squared_error: 0.4514 - val_loss: 0.3404 - val_mean_squared_error: 0.3404\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35701 to 0.34036, saving model to combined3.h5\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.4535 - mean_squared_error: 0.4535 - val_loss: 0.4375 - val_mean_squared_error: 0.4375\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34036\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.4388 - mean_squared_error: 0.4388 - val_loss: 0.6090 - val_mean_squared_error: 0.6090\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34036\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 0.4248 - mean_squared_error: 0.4248 - val_loss: 0.3281 - val_mean_squared_error: 0.3281\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34036 to 0.32813, saving model to combined3.h5\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.4419 - mean_squared_error: 0.4419 - val_loss: 0.4176 - val_mean_squared_error: 0.4176\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32813\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.4386 - mean_squared_error: 0.4386 - val_loss: 0.4205 - val_mean_squared_error: 0.4205\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32813\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.5391 - val_mean_squared_error: 0.5391\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32813\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.4074 - mean_squared_error: 0.4074 - val_loss: 0.4071 - val_mean_squared_error: 0.4071\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32813\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.3793 - mean_squared_error: 0.3793 - val_loss: 0.7270 - val_mean_squared_error: 0.7270\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.32813\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.2816 - val_mean_squared_error: 0.2816\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28158, saving model to combined4.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.3526 - mean_squared_error: 0.3526 - val_loss: 0.2215 - val_mean_squared_error: 0.2215\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28158 to 0.22149, saving model to combined4.h5\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.3973 - mean_squared_error: 0.3973 - val_loss: 0.7177 - val_mean_squared_error: 0.7177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.22149\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.6977 - val_mean_squared_error: 0.6977\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.22149\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.3359 - mean_squared_error: 0.3359 - val_loss: 0.5459 - val_mean_squared_error: 0.5459\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22149\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.2572 - val_mean_squared_error: 0.2572\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22149\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.3444 - mean_squared_error: 0.3444 - val_loss: 0.2749 - val_mean_squared_error: 0.2749\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22149\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "def train_model(fname='model.h5'):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5),\n",
    "        ModelCheckpoint(fname, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    combined_net.fit_generator(train_gen,\n",
    "              epochs=100,\n",
    "                steps_per_epoch=100,\n",
    "            validation_steps=10,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=test_gen[0],\n",
    "              shuffle=True\n",
    "              )\n",
    "    return combined_net\n",
    "\n",
    "for i in range(num_train):\n",
    "    train_model('combined' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def split_model(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('gru').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('gru').output.shape[1], ), name='vector_history_inp')\n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output])\n",
    "    hidden = best_model.get_layer('hidden')(concat)\n",
    "    output = best_model.get_layer('output')(hidden)\n",
    "\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, best_model.get_layer('move_inp').input], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def delete_dir(path):\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "\n",
    "for i in range(5):\n",
    "    combined_file = 'combined' + str(i) + '.h5'\n",
    "    model_folder_name = data_folder + '_model'\n",
    "    \n",
    "    model_folder_path = Path('../models/', model_folder_name)\n",
    "    delete_dir(model_folder_path)\n",
    "    model_folder_path.mkdir()\n",
    "    \n",
    "    split_model(combined_file, model_folder_name)\n",
    "    subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name, 'gs://landlord_ai/models/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
