{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.32.tar.gz (13 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.32-py3-none-any.whl size=18048 sha256=4cc187e46b97490440d6d352dd57856f0ec8f24e7dc360c1f4f65c317dfb1278\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/6c/36/0c/be61a773f6ee6a6673efc46b95fdf97f835e42be8f714f2e19\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "Successfully installed landlord-ai-0.1.32\n",
      "Collecting keras.preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 787 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.2)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from landlordai.game.player import LearningPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreppedDataGenerator(Sequence):\n",
    "    def __init__(self, path_id, batch_size=1024, timesteps_length=LearningPlayer.TIMESTEPS):\n",
    "        self.path_id = path_id\n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps_length = timesteps_length\n",
    "        \n",
    "        self.load_cache()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.all_history_matrices.shape[0] // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        p = np.random.permutation(self.all_history_matrices.shape[0])\n",
    "        \n",
    "        self.all_history_matrices = self.all_history_matrices[p]\n",
    "        self.all_move_vectors = self.all_move_vectors[p]\n",
    "        self.all_hand_vectors = self.all_hand_vectors[p]\n",
    "        self.all_y = self.all_y[p]\n",
    "        \n",
    "    def load_cache(self):\n",
    "        with np.load(self.path_id) as npzfile:\n",
    "            self.all_history_matrices = npzfile['history_matrix']\n",
    "            self.all_move_vectors = npzfile['move_vectors']\n",
    "            self.all_hand_vectors = npzfile['hand_vectors']\n",
    "            self.all_y = npzfile['y']\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        \n",
    "        history_matrices = self.all_history_matrices[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        move_vectors = self.all_move_vectors[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        hand_vectors = self.all_hand_vectors[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        y = self.all_y[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        \n",
    "        #return [self.densify(history_matrices), move_vectors, hand_vectors], y\n",
    "        return [history_matrices, move_vectors, hand_vectors], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '4_11_actualq4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../data/4_11_actualq4_merged': No such file or directory\n",
      "Copying gs://hseokho-lai/4_11_actualq4_merged/test.npz...\n",
      "Copying gs://hseokho-lai/4_11_actualq4_merged/train.npz...                      \n",
      "\\ [2/2 files][ 14.6 GiB/ 14.6 GiB] 100% Done  70.9 MiB/s ETA 00:00:00           \n",
      "Operation completed over 2 objects/14.6 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "assert data_folder is not None\n",
    "!rm -r ../data/{data_folder}_merged\n",
    "!gsutil -m cp -r gs://hseokho-lai/{data_folder}_merged/ ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = PreppedDataGenerator('../data/' + data_folder + '_merged/train.npz', batch_size=1 << 11)\n",
    "test_gen = PreppedDataGenerator('../data/' + data_folder + '_merged/test.npz', batch_size=1 << 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(train_gen[1][0][0], train_gen[0][0][0])\n",
    "assert len(train_gen[0][0]) == 3\n",
    "for i in range(3):\n",
    "    get_set = train_gen[0][0][0]\n",
    "    if len(get_set.shape) != 3:\n",
    "        print(get_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bidi():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 160\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden1 = Dense(384, activation='relu', name='hidden1')(concat)\n",
    "    hidden2 = Dense(160, activation='relu', name='hidden2')(BatchNormalization(name='bn1')(hidden1))\n",
    "    hidden3 = Dense(64, activation='relu', name='hidden3')(BatchNormalization(name='bn2')(hidden2))\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn3')(hidden3))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def split_model_triage(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    split_model(best_model, model_folder)\n",
    "    \n",
    "def split_model(best_model, model_folder):\n",
    "    bn1 = best_model.get_layer('bn1')\n",
    "    bn2 = best_model.get_layer('bn2')\n",
    "    bn3 = best_model.get_layer('bn3')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden1 = best_model.get_layer('hidden1')(concat)\n",
    "    hidden2 = best_model.get_layer('hidden2')(bn1(hidden1))\n",
    "    hidden3 = best_model.get_layer('hidden3')(bn2(hidden2))\n",
    "    output = best_model.get_layer('output')(bn3(hidden3))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))\n",
    "    best_model.save(str(model_folder / 'combined.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_model(combined_file, net_dir):\n",
    "    sanity_set = train_gen[0]\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "    \n",
    "def publish_model(i):\n",
    "    combined_file = data_folder + '_combined_' + str(i) + '.h5'\n",
    "    if os.path.exists(combined_file):\n",
    "        model_folder_name = data_folder + '_model' + str(i)\n",
    "\n",
    "        model_folder_path = Path('../models/', model_folder_name)\n",
    "        delete_dir(model_folder_path)\n",
    "        model_folder_path.mkdir()\n",
    "\n",
    "        split_model_triage(combined_file, model_folder_path)\n",
    "        sanity_check_model(combined_file, model_folder_path)\n",
    "        print(model_folder_name)\n",
    "        subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', 'gs://hseokho-lai/models/' + model_folder_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_revision_model(model_folder):\n",
    "    print('Reloading from', model_folder)\n",
    "    return keras.models.load_model('../models/' + model_folder + '/combined.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading from 4_11_actualq3_model15\n",
      "Epoch 1/50\n",
      "4261/4261 [==============================] - 723s 170ms/step - loss: 0.2785 - mean_squared_error: 0.2785 - val_loss: 0.2729 - val_mean_squared_error: 0.3438\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.34383, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 2/50\n",
      "4261/4261 [==============================] - 722s 169ms/step - loss: 0.2417 - mean_squared_error: 0.2417 - val_loss: 0.2647 - val_mean_squared_error: 0.3008\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.34383 to 0.30077, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 3/50\n",
      "4261/4261 [==============================] - 720s 169ms/step - loss: 0.2218 - mean_squared_error: 0.2218 - val_loss: 0.2578 - val_mean_squared_error: 0.2457\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.30077 to 0.24573, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 4/50\n",
      "4261/4261 [==============================] - 714s 168ms/step - loss: 0.2091 - mean_squared_error: 0.2091 - val_loss: 0.2109 - val_mean_squared_error: 0.2352\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.24573 to 0.23518, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 5/50\n",
      "4261/4261 [==============================] - 714s 168ms/step - loss: 0.2005 - mean_squared_error: 0.2005 - val_loss: 0.2742 - val_mean_squared_error: 0.2678\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.23518\n",
      "Epoch 6/50\n",
      "4261/4261 [==============================] - 715s 168ms/step - loss: 0.1931 - mean_squared_error: 0.1931 - val_loss: 0.1929 - val_mean_squared_error: 0.2027\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.23518 to 0.20266, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 7/50\n",
      "4261/4261 [==============================] - 715s 168ms/step - loss: 0.1869 - mean_squared_error: 0.1869 - val_loss: 0.1665 - val_mean_squared_error: 0.1859\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.20266 to 0.18594, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 8/50\n",
      "4261/4261 [==============================] - 715s 168ms/step - loss: 0.1814 - mean_squared_error: 0.1814 - val_loss: 0.1873 - val_mean_squared_error: 0.1816\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.18594 to 0.18163, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 9/50\n",
      "4261/4261 [==============================] - 715s 168ms/step - loss: 0.1767 - mean_squared_error: 0.1767 - val_loss: 0.2082 - val_mean_squared_error: 0.2104\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.18163\n",
      "Epoch 10/50\n",
      "4261/4261 [==============================] - 716s 168ms/step - loss: 0.1728 - mean_squared_error: 0.1728 - val_loss: 0.1628 - val_mean_squared_error: 0.1702\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.18163 to 0.17024, saving model to 4_11_actualq4_combined_23.h5\n",
      "Epoch 11/50\n",
      "4261/4261 [==============================] - 715s 168ms/step - loss: 0.1690 - mean_squared_error: 0.1690 - val_loss: 0.1803 - val_mean_squared_error: 0.1940\n",
      "\n",
      "Epoch 00011: val_mean_squared_error did not improve from 0.17024\n",
      "Epoch 12/50\n",
      "4261/4261 [==============================] - 720s 169ms/step - loss: 0.1649 - mean_squared_error: 0.1649 - val_loss: 0.1617 - val_mean_squared_error: 0.1912\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 0.17024\n",
      "Epoch 13/50\n",
      "4261/4261 [==============================] - 714s 168ms/step - loss: 0.1624 - mean_squared_error: 0.1624 - val_loss: 0.1928 - val_mean_squared_error: 0.1709\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve from 0.17024\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_11_actualq4_combined_23.h5 0.18474837 0.1847483\n",
      "4_11_actualq4_model23\n",
      "Reloading from 4_11_actualq3_model15\n",
      "Epoch 1/50\n",
      "4261/4261 [==============================] - 707s 166ms/step - loss: 0.2783 - mean_squared_error: 0.2783 - val_loss: 0.2428 - val_mean_squared_error: 0.2490\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.24899, saving model to 4_11_actualq4_combined_24.h5\n",
      "Epoch 2/50\n",
      "4261/4261 [==============================] - 705s 165ms/step - loss: 0.2421 - mean_squared_error: 0.2421 - val_loss: 0.3068 - val_mean_squared_error: 0.3321\n",
      "\n",
      "Epoch 00002: val_mean_squared_error did not improve from 0.24899\n",
      "Epoch 3/50\n",
      "4261/4261 [==============================] - 705s 166ms/step - loss: 0.2216 - mean_squared_error: 0.2216 - val_loss: 0.2206 - val_mean_squared_error: 0.2061\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.24899 to 0.20606, saving model to 4_11_actualq4_combined_24.h5\n",
      "Epoch 4/50\n",
      "4261/4261 [==============================] - 705s 165ms/step - loss: 0.2093 - mean_squared_error: 0.2093 - val_loss: 0.1940 - val_mean_squared_error: 0.2067\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.20606\n",
      "Epoch 5/50\n",
      "4261/4261 [==============================] - 706s 166ms/step - loss: 0.2011 - mean_squared_error: 0.2011 - val_loss: 0.1989 - val_mean_squared_error: 0.2019\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.20606 to 0.20194, saving model to 4_11_actualq4_combined_24.h5\n",
      "Epoch 6/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.1924 - mean_squared_error: 0.1924 - val_loss: 0.2018 - val_mean_squared_error: 0.2096\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 0.20194\n",
      "Epoch 7/50\n",
      "4261/4261 [==============================] - 705s 165ms/step - loss: 0.1872 - mean_squared_error: 0.1872 - val_loss: 0.2095 - val_mean_squared_error: 0.1789\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.20194 to 0.17888, saving model to 4_11_actualq4_combined_24.h5\n",
      "Epoch 8/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.1824 - mean_squared_error: 0.1824 - val_loss: 0.1955 - val_mean_squared_error: 0.1902\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 0.17888\n",
      "Epoch 9/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.1778 - mean_squared_error: 0.1778 - val_loss: 0.1778 - val_mean_squared_error: 0.1921\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.17888\n",
      "Epoch 10/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.1725 - mean_squared_error: 0.1725 - val_loss: 0.1888 - val_mean_squared_error: 0.1973\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 0.17888\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_11_actualq4_combined_24.h5 0.18960169 0.1896017\n",
      "4_11_actualq4_model24\n",
      "Reloading from 4_11_actualq3_model15\n",
      "Epoch 1/50\n",
      "4261/4261 [==============================] - 706s 166ms/step - loss: 0.2806 - mean_squared_error: 0.2806 - val_loss: 0.5889 - val_mean_squared_error: 0.5181\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.51811, saving model to 4_11_actualq4_combined_25.h5\n",
      "Epoch 2/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.2395 - mean_squared_error: 0.2395 - val_loss: 0.2807 - val_mean_squared_error: 0.2527\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.51811 to 0.25270, saving model to 4_11_actualq4_combined_25.h5\n",
      "Epoch 3/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.2243 - mean_squared_error: 0.2243 - val_loss: 0.1858 - val_mean_squared_error: 0.2157\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.25270 to 0.21575, saving model to 4_11_actualq4_combined_25.h5\n",
      "Epoch 4/50\n",
      "4261/4261 [==============================] - 704s 165ms/step - loss: 0.2094 - mean_squared_error: 0.2094 - val_loss: 0.1905 - val_mean_squared_error: 0.2017\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.21575 to 0.20168, saving model to 4_11_actualq4_combined_25.h5\n",
      "Epoch 5/50\n",
      "4261/4261 [==============================] - 695s 163ms/step - loss: 0.2006 - mean_squared_error: 0.2006 - val_loss: 0.2553 - val_mean_squared_error: 0.2229\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.20168\n",
      "Epoch 6/50\n",
      "3623/4261 [========================>.....] - ETA: 1:40 - loss: 0.1934 - mean_squared_error: 0.1934"
     ]
    }
   ],
   "source": [
    "def train_model(fname=None, combined_net=None):\n",
    "    if not combined_net:\n",
    "        combined_net = create_model_bidi()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=3),\n",
    "        ModelCheckpoint(fname, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    combined_net.fit_generator(train_gen,\n",
    "                epochs=50,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=test_gen,\n",
    "                shuffle=True,\n",
    "                workers=1,\n",
    "                max_queue_size=10,\n",
    "                use_multiprocessing=False\n",
    "              )\n",
    "    return combined_net\n",
    "\n",
    "for i in range(23, 30):\n",
    "    loaded_model = load_revision_model('4_11_actualq3_model15')\n",
    "    train_model(data_folder + '_combined_' + str(i) + '.h5', loaded_model)\n",
    "    publish_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_7_actualq3_combined_10.h5 0.17735313047530865 0.1773531122255772\n",
      "4_7_actualq3_model10\n"
     ]
    }
   ],
   "source": [
    "publish_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
