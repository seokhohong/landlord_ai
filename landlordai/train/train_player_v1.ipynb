{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.16.tar.gz (10 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.16-py3-none-any.whl size=14900 sha256=71389c3b96dc9e234101f890bd072a385c97d87361591701e789ad4fe6f52f54\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f2/30/a1/6679bde3a13e99296208bc091c3494be0e7ec695f2382e7894\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "  Attempting uninstall: landlord-ai\n",
      "    Found existing installation: landlord-ai 0.1.14\n",
      "    Uninstalling landlord-ai-0.1.14:\n",
      "      Successfully uninstalled landlord-ai-0.1.14\n",
      "Successfully installed landlord-ai-0.1.16\n",
      "Collecting keras.preprocessing\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from landlordai.game.player import LearningPlayer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_ids, batch_size=1024, shuffle=True, clamp=False, timesteps_length=LearningPlayer_v1.TIMESTEPS):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.path_ids = path_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.clamp = clamp\n",
    "        self.timesteps_length = timesteps_length\n",
    "        \n",
    "        self.load_cache()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return 1000\n",
    "\n",
    "    def load_cache(self):\n",
    "        with open(random.choice(self.path_ids), 'rb') as f:\n",
    "            history_matrices, move_vectors, hand_vectors, y = pickle.load(f)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                p = np.random.permutation(len(history_matrices))\n",
    "                \n",
    "                history_matrices = np.array(history_matrices)[p]\n",
    "                move_vectors = move_vectors[p]\n",
    "                hand_vectors = hand_vectors[p]\n",
    "                y = y[p]\n",
    "        \n",
    "        # unflatten\n",
    "        history_matrices = self.densify(history_matrices)\n",
    "\n",
    "        self.cache = (history_matrices, move_vectors, hand_vectors, y) \n",
    "        self.curr_index = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        limit = min(len(self.cache[0]), (self.curr_index + 1) * self.batch_size)\n",
    "        \n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        history_matrices = self.cache[0][self.curr_index * self.batch_size: limit]\n",
    "        move_vectors = self.cache[1][self.curr_index * self.batch_size: limit]\n",
    "        hand_vectors = self.cache[2][self.curr_index * self.batch_size: limit]\n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        y = self.cache[3][self.curr_index * self.batch_size: limit]\n",
    "        self.curr_index += 1\n",
    "        \n",
    "        # load a new batch\n",
    "        if (self.curr_index + 1) * self.batch_size >= len(self.cache[0]):\n",
    "            self.load_cache()\n",
    "        \n",
    "        return [history_matrices, move_vectors, hand_vectors], self.adjust_y(y)\n",
    "\n",
    "    def densify(self, sparse_matrix):\n",
    "        return np.array([x.todense()[:self.timesteps_length] for x in sparse_matrix])\n",
    "\n",
    "    def adjust_y(self, y):\n",
    "        if not self.clamp:\n",
    "            return y\n",
    "        new_y = []\n",
    "        for elem in y:\n",
    "            if abs(int(elem) - elem) > 1E-4:\n",
    "                new_y.append(0)\n",
    "            else:\n",
    "                new_y.append(elem)\n",
    "        return np.array(new_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '4_1_sim3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://landlord_ai/4_1_sim3/0.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/1.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/10.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/11.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/12.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/13.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/14.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/15.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/16.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/17.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/18.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/2.pkl...one                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/19.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/21.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/20.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/22.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/23.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/24.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/25.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/4.pkl...\n",
      "Copying gs://landlord_ai/4_1_sim3/26.pkl...ne                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/5.pkl...one                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/6.pkl...one                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/7.pkl...one                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/8.pkl...one                                   \n",
      "Copying gs://landlord_ai/4_1_sim3/9.pkl...one                                   \n",
      "| [26/26 files][  2.0 GiB/  2.0 GiB] 100% Done     0.0 B/s                      \n",
      "Operation completed over 26 objects/2.0 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "assert data_folder is not None\n",
    "!rm -r ../data/$data_folder\n",
    "!gsutil -m cp -r gs://landlord_ai/$data_folder ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '../data/' + data_folder\n",
    "filenames = [directory + '/' + file for file in os.listdir(directory)]\n",
    "\n",
    "if len(filenames) == 1:\n",
    "    train_path_ids = filenames\n",
    "    test_path_ids = filenames\n",
    "else:\n",
    "    divider = int(len(filenames) * 0.9)\n",
    "    train_path_ids = filenames[:divider]\n",
    "    test_path_ids = filenames[divider:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality control\n",
    "inspector = DataGenerator(train_path_ids, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 17.96it/s]\n"
     ]
    }
   ],
   "source": [
    "irregularities = 0\n",
    "for j in tqdm(range(100)):\n",
    "    a_set = inspector[j]\n",
    "    for i in range(1000):\n",
    "        history = a_set[0][0][i]\n",
    "        history_sum = np.sum(history)\n",
    "        q = a_set[1][i]\n",
    "        irregular = abs(q) > 1 and history_sum < 20\n",
    "        if irregular:\n",
    "            #with open('irregular.pkl', 'wb') as f:\n",
    "            #    pickle.dump((a_set, i), f)\n",
    "            irregularities += 1\n",
    "            print(history, q)\n",
    "        #assert not (abs(q) > 1 and history_sum < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(train_path_ids, timesteps_length=50)\n",
    "test_gen = DataGenerator(test_path_ids, timesteps_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(train_gen[0][0][0], train_gen[0][0][0])\n",
    "assert len(train_gen[0][0]) == 3\n",
    "for i in range(3):\n",
    "    get_set = train_gen[0][0][0]\n",
    "    if len(get_set.shape) != 3:\n",
    "        print(get_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bidi():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 32\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer_v1.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer_v1.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer_v1.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden = Dense(32, activation='relu', name='hidden')(concat)\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn')(hidden))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def split_model(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    if has_layer(best_model, 'bn2'):\n",
    "        split_model_2(best_model, model_folder)\n",
    "    elif has_layer(best_model, 'bidi'):\n",
    "        split_model_bidi(best_model, model_folder)\n",
    "    else:\n",
    "        split_model_1(best_model, model_folder)\n",
    "    \n",
    "def split_model_bidi(best_model, model_folder):\n",
    "    bn = best_model.get_layer('bn')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden = best_model.get_layer('hidden')(concat)\n",
    "    output = best_model.get_layer('output')(bn(hidden))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_model(combined_file, net_dir):\n",
    "    sanity_set = train_gen[0]\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer_v1(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "    \n",
    "def publish_model(i):\n",
    "    combined_file = data_folder + '_combined_' + str(i) + '.h5'\n",
    "    if os.path.exists(combined_file):\n",
    "        model_folder_name = data_folder + '_model' + str(i)\n",
    "\n",
    "        model_folder_path = Path('../models/', model_folder_name)\n",
    "        delete_dir(model_folder_path)\n",
    "        model_folder_path.mkdir()\n",
    "\n",
    "        split_model(combined_file, model_folder_path)\n",
    "        sanity_check_model(combined_file, model_folder_path)\n",
    "        print(model_folder_name)\n",
    "        subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', 'gs://landlord_ai/models/' + model_folder_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   2/2000 [..............................] - ETA: 1:04:24 - loss: 4.7898 - mean_squared_error: 4.7898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.480550). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 292s 146ms/step - loss: 0.8816 - mean_squared_error: 0.8814 - val_loss: 0.3146 - val_mean_squared_error: 0.7094\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.70940, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 286s 143ms/step - loss: 0.5180 - mean_squared_error: 0.5180 - val_loss: 0.4422 - val_mean_squared_error: 0.6416\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.70940 to 0.64162, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 289s 144ms/step - loss: 0.4432 - mean_squared_error: 0.4432 - val_loss: 0.4311 - val_mean_squared_error: 0.6006\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.64162 to 0.60061, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 285s 142ms/step - loss: 0.3856 - mean_squared_error: 0.3857 - val_loss: 0.4870 - val_mean_squared_error: 0.4622\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.60061 to 0.46223, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 284s 142ms/step - loss: 0.3534 - mean_squared_error: 0.3535 - val_loss: 0.3261 - val_mean_squared_error: 0.4053\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.46223 to 0.40527, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 284s 142ms/step - loss: 0.3225 - mean_squared_error: 0.3225 - val_loss: 0.2808 - val_mean_squared_error: 0.3748\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.40527 to 0.37480, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 287s 144ms/step - loss: 0.3023 - mean_squared_error: 0.3023 - val_loss: 0.2987 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.37480 to 0.36284, saving model to 4_1_sim3_combined_8.h5\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 286s 143ms/step - loss: 0.2930 - mean_squared_error: 0.2930 - val_loss: 0.2728 - val_mean_squared_error: 0.3697\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 0.36284\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 288s 144ms/step - loss: 0.2716 - mean_squared_error: 0.2717 - val_loss: 0.3556 - val_mean_squared_error: 0.3706\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.36284\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected vector_history_inp to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-faa651533b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_combined_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpublish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c95e83865d01>\u001b[0m in \u001b[0;36mpublish_model\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msplit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msanity_check_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gsutil'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_folder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gs://landlord_ai/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_folder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8799dabe7466>\u001b[0m in \u001b[0;36msanity_check_model\u001b[0;34m(combined_file, net_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0merror_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcomposite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/landlordai/game/player.py\u001b[0m in \u001b[0;36mget_position_predictions\u001b[0;34m(self, history_matrix, move_options_matrix, hand_matrix)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_options_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_options_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_options_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecide_best_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTurnPosition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected vector_history_inp to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "def train_model(fname='model.h5'):\n",
    "    combined_net = create_model_bidi()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=2),\n",
    "        ModelCheckpoint(fname, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    combined_net.fit_generator(train_gen,\n",
    "              epochs=50,\n",
    "                steps_per_epoch=2000,\n",
    "                validation_steps=200,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=test_gen,\n",
    "                shuffle=True,\n",
    "                workers=2,\n",
    "                max_queue_size=200,\n",
    "                use_multiprocessing=False\n",
    "              )\n",
    "    return combined_net\n",
    "\n",
    "for i in range(8, 10):\n",
    "    train_model(data_folder + '_combined_' + str(i) + '.h5')\n",
    "    publish_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_1_sim3_combined_8.h5 0.2535367960371706 0.2535367960371706\n",
      "4_1_sim3_model8\n"
     ]
    }
   ],
   "source": [
    "publish_model(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
