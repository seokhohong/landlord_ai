{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.18.tar.gz (11 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.18-py3-none-any.whl size=15269 sha256=7cb7e3bc54d4cef08302e22030c41559bd90471345bdfb8beb66a81c311272f8\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/cf/c3/72/f557fc91b3268ac381bbfcb1ef61936da045134fa201ca6e93\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "Successfully installed landlord-ai-0.1.18\n",
      "Collecting keras.preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 541 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.2)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from landlordai.game.player import LearningPlayer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_ids, batch_size=1024, shuffle=True, clamp=False, timesteps_length=LearningPlayer_v1.TIMESTEPS):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.path_ids = path_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.clamp = clamp\n",
    "        self.timesteps_length = timesteps_length\n",
    "        \n",
    "        self.load_cache()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return 1000\n",
    "\n",
    "    def load_cache(self):\n",
    "        with open(random.choice(self.path_ids), 'rb') as f:\n",
    "            history_matrices, move_vectors, hand_vectors, y = pickle.load(f)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                p = np.random.permutation(len(history_matrices))\n",
    "                \n",
    "                history_matrices = np.array(history_matrices)[p]\n",
    "                move_vectors = move_vectors[p]\n",
    "                hand_vectors = hand_vectors[p]\n",
    "                y = y[p]\n",
    "        \n",
    "        # unflatten\n",
    "        history_matrices = self.densify(history_matrices)\n",
    "\n",
    "        self.cache = (history_matrices, move_vectors, hand_vectors, y) \n",
    "        self.curr_index = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        limit = min(len(self.cache[0]), (self.curr_index + 1) * self.batch_size)\n",
    "        \n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        history_matrices = self.cache[0][self.curr_index * self.batch_size: limit]\n",
    "        move_vectors = self.cache[1][self.curr_index * self.batch_size: limit]\n",
    "        hand_vectors = self.cache[2][self.curr_index * self.batch_size: limit]\n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        y = self.cache[3][self.curr_index * self.batch_size: limit]\n",
    "        self.curr_index += 1\n",
    "        \n",
    "        # load a new batch\n",
    "        if (self.curr_index + 1) * self.batch_size >= len(self.cache[0]):\n",
    "            self.load_cache()\n",
    "        \n",
    "        return [history_matrices, move_vectors, hand_vectors], self.adjust_y(y)\n",
    "\n",
    "    def densify(self, sparse_matrix):\n",
    "        return np.array([x.todense()[:self.timesteps_length] for x in sparse_matrix])\n",
    "\n",
    "    def adjust_y(self, y):\n",
    "        if not self.clamp:\n",
    "            return y\n",
    "        new_y = []\n",
    "        for elem in y:\n",
    "            if abs(int(elem) - elem) > 1E-4:\n",
    "                new_y.append(0)\n",
    "            else:\n",
    "                new_y.append(elem)\n",
    "        return np.array(new_y)\n",
    "\n",
    "    \n",
    "class PreppedDataGenerator(Sequence):\n",
    "    def __init__(self, path_id, batch_size=1024, timesteps_length=LearningPlayer_v1.TIMESTEPS):\n",
    "        self.path_id = path_id\n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps_length = timesteps_length\n",
    "        \n",
    "        self.load_cache()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache[0]) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        p = np.random.permutation(len(self.cache[0]))\n",
    "        \n",
    "        all_history_matrices = self.cache[0][p]\n",
    "        all_move_vectors = self.cache[1][p]\n",
    "        all_hand_vectors = self.cache[2][p]\n",
    "        all_y = self.cache[3][p]\n",
    "        \n",
    "        self.cache = (all_history_matrices, all_move_vectors, all_hand_vectors, all_y)\n",
    "        \n",
    "    def load_cache(self):\n",
    "        with open(self.path_id, 'rb') as f:\n",
    "            history_matrices, move_vectors, hand_vectors, y = pickle.load(f)\n",
    "            \n",
    "        self.cache = (history_matrices, move_vectors, hand_vectors, np.array(y)) \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        \n",
    "        history_matrices = self.cache[0][index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        move_vectors = self.cache[1][index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        hand_vectors = self.cache[2][index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        y = self.cache[3][index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        \n",
    "        #return [self.densify(history_matrices), move_vectors, hand_vectors], y\n",
    "        return [history_matrices, move_vectors, hand_vectors], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '4_2_sim4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data_folder is not None\n",
    "!rm -r ../data/{data_folder}_agg\n",
    "!gsutil -m cp -r gs://landlord_ai/{data_folder}_agg/ ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = PreppedDataGenerator('../data/' + data_folder + '_agg/train.pkl', batch_size=1 << 14, timesteps_length=50)\n",
    "test_gen = PreppedDataGenerator('../data/' + data_folder + '_agg/test.pkl', batch_size=1 << 14, timesteps_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(train_gen[1][0][0], train_gen[0][0][0])\n",
    "assert len(train_gen[0][0]) == 3\n",
    "for i in range(3):\n",
    "    get_set = train_gen[0][0][0]\n",
    "    if len(get_set.shape) != 3:\n",
    "        print(get_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bidi():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 96\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer_v1.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer_v1.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer_v1.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden1 = Dense(64, activation='relu', name='hidden1')(concat)\n",
    "    hidden2 = Dense(32, activation='relu', name='hidden2')(BatchNormalization(name='bn1')(hidden1))\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn2')(hidden2))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def split_model_triage(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    split_model(best_model, model_folder)\n",
    "    \n",
    "def split_model(best_model, model_folder):\n",
    "    bn1 = best_model.get_layer('bn1')\n",
    "    bn2 = best_model.get_layer('bn2')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden1 = best_model.get_layer('hidden1')(concat)\n",
    "    hidden2 = best_model.get_layer('hidden2')(bn1(hidden1))\n",
    "    output = best_model.get_layer('output')(bn2(hidden2))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_model(combined_file, net_dir):\n",
    "    sanity_set = train_gen[0]\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer_v1(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "    \n",
    "def publish_model(i):\n",
    "    combined_file = data_folder + '_combined_' + str(i) + '.h5'\n",
    "    if os.path.exists(combined_file):\n",
    "        model_folder_name = data_folder + '_model' + str(i)\n",
    "\n",
    "        model_folder_path = Path('../models/', model_folder_name)\n",
    "        delete_dir(model_folder_path)\n",
    "        model_folder_path.mkdir()\n",
    "\n",
    "        split_model_triage(combined_file, model_folder_path)\n",
    "        sanity_check_model(combined_file, model_folder_path)\n",
    "        print(model_folder_name)\n",
    "        subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', 'gs://landlord_ai/models/' + model_folder_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.7768 - mean_squared_error: 0.7768 - val_loss: 0.6630 - val_mean_squared_error: 0.6585\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.65847, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.3795 - mean_squared_error: 0.3795 - val_loss: 0.4031 - val_mean_squared_error: 0.3940\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.65847 to 0.39395, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.3400 - mean_squared_error: 0.3400 - val_loss: 0.3391 - val_mean_squared_error: 0.3374\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.39395 to 0.33736, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.3205 - mean_squared_error: 0.3205 - val_loss: 0.2988 - val_mean_squared_error: 0.3148\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.33736 to 0.31476, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.3073 - mean_squared_error: 0.3073 - val_loss: 0.2985 - val_mean_squared_error: 0.3111\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.31476 to 0.31108, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2883 - mean_squared_error: 0.2883 - val_loss: 0.3087 - val_mean_squared_error: 0.2970\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.31037 to 0.29697, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.2802 - mean_squared_error: 0.2802 - val_loss: 0.3214 - val_mean_squared_error: 0.3144\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 0.29697\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.2736 - mean_squared_error: 0.2736 - val_loss: 0.2863 - val_mean_squared_error: 0.2990\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.29697\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2672 - mean_squared_error: 0.2672 - val_loss: 0.2733 - val_mean_squared_error: 0.2743\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.29697 to 0.27428, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.2612 - mean_squared_error: 0.2612 - val_loss: 0.2713 - val_mean_squared_error: 0.2718\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.27428 to 0.27177, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2543 - mean_squared_error: 0.2543 - val_loss: 0.3009 - val_mean_squared_error: 0.2883\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 0.27177\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.2481 - mean_squared_error: 0.2481 - val_loss: 0.2671 - val_mean_squared_error: 0.2628\n",
      "\n",
      "Epoch 00013: val_mean_squared_error improved from 0.27177 to 0.26276, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.2420 - mean_squared_error: 0.2420 - val_loss: 0.2450 - val_mean_squared_error: 0.2532\n",
      "\n",
      "Epoch 00014: val_mean_squared_error improved from 0.26276 to 0.25321, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.2361 - mean_squared_error: 0.2361 - val_loss: 0.2542 - val_mean_squared_error: 0.2559\n",
      "\n",
      "Epoch 00015: val_mean_squared_error did not improve from 0.25321\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.2311 - mean_squared_error: 0.2311 - val_loss: 0.2441 - val_mean_squared_error: 0.2445\n",
      "\n",
      "Epoch 00016: val_mean_squared_error improved from 0.25321 to 0.24447, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.2263 - mean_squared_error: 0.2263 - val_loss: 0.2399 - val_mean_squared_error: 0.2371\n",
      "\n",
      "Epoch 00017: val_mean_squared_error improved from 0.24447 to 0.23714, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.2220 - mean_squared_error: 0.2220 - val_loss: 0.2405 - val_mean_squared_error: 0.2361\n",
      "\n",
      "Epoch 00018: val_mean_squared_error improved from 0.23714 to 0.23609, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.2181 - mean_squared_error: 0.2181 - val_loss: 0.2274 - val_mean_squared_error: 0.2286\n",
      "\n",
      "Epoch 00019: val_mean_squared_error improved from 0.23609 to 0.22856, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.2143 - mean_squared_error: 0.2143 - val_loss: 0.2270 - val_mean_squared_error: 0.2290\n",
      "\n",
      "Epoch 00020: val_mean_squared_error did not improve from 0.22856\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.2112 - mean_squared_error: 0.2112 - val_loss: 0.2191 - val_mean_squared_error: 0.2232\n",
      "\n",
      "Epoch 00021: val_mean_squared_error improved from 0.22856 to 0.22316, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.2079 - mean_squared_error: 0.2079 - val_loss: 0.2177 - val_mean_squared_error: 0.2224\n",
      "\n",
      "Epoch 00022: val_mean_squared_error improved from 0.22316 to 0.22236, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.2055 - mean_squared_error: 0.2055 - val_loss: 0.2221 - val_mean_squared_error: 0.2197\n",
      "\n",
      "Epoch 00023: val_mean_squared_error improved from 0.22236 to 0.21971, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.2026 - mean_squared_error: 0.2026 - val_loss: 0.2216 - val_mean_squared_error: 0.2252\n",
      "\n",
      "Epoch 00024: val_mean_squared_error did not improve from 0.21971\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.2006 - mean_squared_error: 0.2006 - val_loss: 0.2166 - val_mean_squared_error: 0.2185\n",
      "\n",
      "Epoch 00025: val_mean_squared_error improved from 0.21971 to 0.21852, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.1982 - mean_squared_error: 0.1982 - val_loss: 0.2131 - val_mean_squared_error: 0.2149\n",
      "\n",
      "Epoch 00026: val_mean_squared_error improved from 0.21852 to 0.21487, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.1959 - mean_squared_error: 0.1959 - val_loss: 0.2122 - val_mean_squared_error: 0.2118\n",
      "\n",
      "Epoch 00027: val_mean_squared_error improved from 0.21487 to 0.21182, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.1945 - mean_squared_error: 0.1945 - val_loss: 0.2051 - val_mean_squared_error: 0.2110\n",
      "\n",
      "Epoch 00028: val_mean_squared_error improved from 0.21182 to 0.21105, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.1923 - mean_squared_error: 0.1923 - val_loss: 0.2083 - val_mean_squared_error: 0.2089\n",
      "\n",
      "Epoch 00029: val_mean_squared_error improved from 0.21105 to 0.20891, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.1906 - mean_squared_error: 0.1906 - val_loss: 0.2082 - val_mean_squared_error: 0.2071\n",
      "\n",
      "Epoch 00030: val_mean_squared_error improved from 0.20891 to 0.20710, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.1890 - mean_squared_error: 0.1890 - val_loss: 0.2074 - val_mean_squared_error: 0.2039\n",
      "\n",
      "Epoch 00031: val_mean_squared_error improved from 0.20710 to 0.20387, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.1873 - mean_squared_error: 0.1873 - val_loss: 0.2040 - val_mean_squared_error: 0.2023\n",
      "\n",
      "Epoch 00032: val_mean_squared_error improved from 0.20387 to 0.20232, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.1860 - mean_squared_error: 0.1860 - val_loss: 0.2025 - val_mean_squared_error: 0.2053\n",
      "\n",
      "Epoch 00033: val_mean_squared_error did not improve from 0.20232\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.1844 - mean_squared_error: 0.1844 - val_loss: 0.2076 - val_mean_squared_error: 0.2030\n",
      "\n",
      "Epoch 00034: val_mean_squared_error did not improve from 0.20232\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1833 - mean_squared_error: 0.1833 - val_loss: 0.1959 - val_mean_squared_error: 0.1997\n",
      "\n",
      "Epoch 00035: val_mean_squared_error improved from 0.20232 to 0.19971, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1819 - mean_squared_error: 0.1819 - val_loss: 0.2043 - val_mean_squared_error: 0.2005\n",
      "\n",
      "Epoch 00036: val_mean_squared_error did not improve from 0.19971\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.1810 - mean_squared_error: 0.1810 - val_loss: 0.2034 - val_mean_squared_error: 0.2026\n",
      "\n",
      "Epoch 00037: val_mean_squared_error did not improve from 0.19971\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.1797 - mean_squared_error: 0.1797 - val_loss: 0.1991 - val_mean_squared_error: 0.1990\n",
      "\n",
      "Epoch 00038: val_mean_squared_error improved from 0.19971 to 0.19900, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1787 - mean_squared_error: 0.1787 - val_loss: 0.2099 - val_mean_squared_error: 0.2033\n",
      "\n",
      "Epoch 00039: val_mean_squared_error did not improve from 0.19900\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1778 - mean_squared_error: 0.1778 - val_loss: 0.1968 - val_mean_squared_error: 0.2006\n",
      "\n",
      "Epoch 00040: val_mean_squared_error did not improve from 0.19900\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1765 - mean_squared_error: 0.1765 - val_loss: 0.1922 - val_mean_squared_error: 0.1971\n",
      "\n",
      "Epoch 00041: val_mean_squared_error improved from 0.19900 to 0.19707, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1759 - mean_squared_error: 0.1759 - val_loss: 0.1989 - val_mean_squared_error: 0.1973\n",
      "\n",
      "Epoch 00042: val_mean_squared_error did not improve from 0.19707\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1749 - mean_squared_error: 0.1749 - val_loss: 0.1971 - val_mean_squared_error: 0.1932\n",
      "\n",
      "Epoch 00043: val_mean_squared_error improved from 0.19707 to 0.19324, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1741 - mean_squared_error: 0.1741 - val_loss: 0.2040 - val_mean_squared_error: 0.1954\n",
      "\n",
      "Epoch 00044: val_mean_squared_error did not improve from 0.19324\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1734 - mean_squared_error: 0.1734 - val_loss: 0.1949 - val_mean_squared_error: 0.1944\n",
      "\n",
      "Epoch 00045: val_mean_squared_error did not improve from 0.19324\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1726 - mean_squared_error: 0.1726 - val_loss: 0.1944 - val_mean_squared_error: 0.1920\n",
      "\n",
      "Epoch 00046: val_mean_squared_error improved from 0.19324 to 0.19201, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1722 - mean_squared_error: 0.1722 - val_loss: 0.1953 - val_mean_squared_error: 0.1909\n",
      "\n",
      "Epoch 00047: val_mean_squared_error improved from 0.19201 to 0.19090, saving model to 4_2_sim4_combined_0.h5\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1713 - mean_squared_error: 0.1713 - val_loss: 0.1870 - val_mean_squared_error: 0.1917\n",
      "\n",
      "Epoch 00048: val_mean_squared_error did not improve from 0.19090\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.1707 - mean_squared_error: 0.1707 - val_loss: 0.1908 - val_mean_squared_error: 0.1930\n",
      "\n",
      "Epoch 00049: val_mean_squared_error did not improve from 0.19090\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1701 - mean_squared_error: 0.1701 - val_loss: 0.1857 - val_mean_squared_error: 0.1907\n",
      "\n",
      "Epoch 00050: val_mean_squared_error improved from 0.19090 to 0.19069, saving model to 4_2_sim4_combined_0.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_2_sim4_combined_0.h5 0.17558248987090347 0.17558248987090347\n",
      "4_2_sim4_model0\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.7155 - mean_squared_error: 0.7155 - val_loss: 0.4575 - val_mean_squared_error: 0.4501\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.45009, saving model to 4_2_sim4_combined_1.h5\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.3714 - mean_squared_error: 0.3714 - val_loss: 0.3693 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.45009 to 0.35870, saving model to 4_2_sim4_combined_1.h5\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.3358 - mean_squared_error: 0.3358 - val_loss: 0.3764 - val_mean_squared_error: 0.3896\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 0.35870\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.3186 - mean_squared_error: 0.3186 - val_loss: 0.3741 - val_mean_squared_error: 0.3715\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.35870\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.3067 - mean_squared_error: 0.3067 - val_loss: 0.3812 - val_mean_squared_error: 0.3811\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.35870\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_2_sim4_combined_1.h5 0.35193981065820845 0.35193981065820845\n",
      "4_2_sim4_model1\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.7697 - mean_squared_error: 0.7697 - val_loss: 0.6785 - val_mean_squared_error: 0.6833\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.68331, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.3882 - mean_squared_error: 0.3882 - val_loss: 0.4289 - val_mean_squared_error: 0.4190\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.68331 to 0.41899, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.3450 - mean_squared_error: 0.3450 - val_loss: 0.3722 - val_mean_squared_error: 0.3730\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.41899 to 0.37301, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.4116 - val_mean_squared_error: 0.4119\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.37301\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.3104 - mean_squared_error: 0.3104 - val_loss: 0.3097 - val_mean_squared_error: 0.3133\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.37301 to 0.31333, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.2999 - mean_squared_error: 0.2999 - val_loss: 0.3227 - val_mean_squared_error: 0.3187\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 0.31333\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.2917 - mean_squared_error: 0.2917 - val_loss: 0.3153 - val_mean_squared_error: 0.3099\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.31333 to 0.30992, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.2845 - mean_squared_error: 0.2845 - val_loss: 0.2855 - val_mean_squared_error: 0.2983\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.30992 to 0.29831, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.2785 - mean_squared_error: 0.2785 - val_loss: 0.2922 - val_mean_squared_error: 0.2899\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 0.29831 to 0.28987, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2719 - mean_squared_error: 0.2719 - val_loss: 0.3034 - val_mean_squared_error: 0.3023\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 0.28987\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.2665 - mean_squared_error: 0.2665 - val_loss: 0.2954 - val_mean_squared_error: 0.2879\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.28987 to 0.28786, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2994 - val_mean_squared_error: 0.3012\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 0.28786\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.2558 - mean_squared_error: 0.2558 - val_loss: 0.2951 - val_mean_squared_error: 0.2961\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve from 0.28786\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.2511 - mean_squared_error: 0.2511 - val_loss: 0.2489 - val_mean_squared_error: 0.2610\n",
      "\n",
      "Epoch 00014: val_mean_squared_error improved from 0.28786 to 0.26100, saving model to 4_2_sim4_combined_2.h5\n",
      "Epoch 15/50\n",
      " 135/1000 [===>..........................] - ETA: 4:13 - loss: 0.2467 - mean_squared_error: 0.2467"
     ]
    }
   ],
   "source": [
    "def train_model(fname='model.h5'):\n",
    "    combined_net = create_model_bidi()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=3),\n",
    "        ModelCheckpoint(fname, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    combined_net.fit_generator(train_gen,\n",
    "                                steps_per_epoch=1000,\n",
    "                epochs=50,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=test_gen,\n",
    "                shuffle=True,\n",
    "                workers=1,\n",
    "                max_queue_size=10,\n",
    "                use_multiprocessing=False\n",
    "              )\n",
    "    return combined_net\n",
    "\n",
    "for i in range(7):\n",
    "    train_model(data_folder + '_combined_' + str(i) + '.h5')\n",
    "    publish_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_2_sim3_combined_1.h5 0.21737819504043962 0.21737819504043962\n",
      "4_2_sim3_model1\n"
     ]
    }
   ],
   "source": [
    "publish_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
