{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.8.tar.gz (10 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.8-py3-none-any.whl size=14153 sha256=eaa6f50d753ec67cb46b59e676f4869c97c8d289ceff6f999cb8ab26ffc3ffb0\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ae/97/17/57fe3a2136f43f8ce1b7cd176a36203ff4c8b5402f3f2815f5\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "Successfully installed landlord-ai-0.1.8\n",
      "Collecting keras.preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 793 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.1)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from landlordai.game.player import LearningPlayer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_ids, batch_size=1024, shuffle=True, clamp=False):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.path_ids = path_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.clamp = clamp\n",
    "        \n",
    "        self.load_cache()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return 1000\n",
    "\n",
    "    def load_cache(self):\n",
    "        with open(random.choice(self.path_ids), 'rb') as f:\n",
    "            history_matrices, move_vectors, hand_vectors, y = pickle.load(f)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                p = np.random.permutation(len(history_matrices))\n",
    "                \n",
    "                history_matrices = np.array(history_matrices)[p]\n",
    "                move_vectors = move_vectors[p]\n",
    "                hand_vectors = hand_vectors[p]\n",
    "                y = y[p]\n",
    "\n",
    "        self.cache = (history_matrices, move_vectors, hand_vectors, y) \n",
    "        self.curr_index = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        limit = min(len(self.cache[0]), (self.curr_index + 1) * self.batch_size)\n",
    "        \n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        history_matrices = self.cache[0][self.curr_index * self.batch_size: limit]\n",
    "        move_vectors = self.cache[1][self.curr_index * self.batch_size: limit]\n",
    "        hand_vectors = self.cache[2][self.curr_index * self.batch_size: limit]\n",
    "        #print(self.curr_index * self.batch_size, limit)\n",
    "        y = self.cache[3][self.curr_index * self.batch_size: limit]\n",
    "        self.curr_index += 1\n",
    "        \n",
    "        # load a new batch\n",
    "        if (self.curr_index + 1) * self.batch_size >= len(self.cache[0]):\n",
    "            print('Reload')\n",
    "            self.load_cache()\n",
    "        \n",
    "        return [self.densify(history_matrices), move_vectors, hand_vectors], self.adjust_y(y)\n",
    "\n",
    "    def densify(self, sparse_matrix):\n",
    "        return np.array([x.todense() for x in sparse_matrix])\n",
    "\n",
    "    def adjust_y(self, y):\n",
    "        if not self.clamp:\n",
    "            return y\n",
    "        new_y = []\n",
    "        for elem in y:\n",
    "            if abs(int(elem) - elem) > 1E-4:\n",
    "                new_y.append(0)\n",
    "            else:\n",
    "                new_y.append(elem)\n",
    "        return np.array(new_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '3_29_sim11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../data/3_29_sim11': No such file or directory\n",
      "Copying gs://landlord_ai/3_29_sim11/0.pkl...\n",
      "Copying gs://landlord_ai/3_29_sim11/1.pkl...                                    \n",
      "Copying gs://landlord_ai/3_29_sim11/2.pkl...                                    \n",
      "Copying gs://landlord_ai/3_29_sim11/3.pkl...                                    \n",
      "Copying gs://landlord_ai/3_29_sim11/4.pkl...                                    \n",
      "\\ [5/5 files][661.0 MiB/661.0 MiB] 100% Done                                    \n",
      "Operation completed over 5 objects/661.0 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "assert data_folderr is not None\n",
    "!rm -r ../data/$data_folder\n",
    "!gsutil -m cp -r gs://landlord_ai/$data_folder ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '../data/' + data_folder\n",
    "filenames = [directory + '/' + file for file in os.listdir(directory)]\n",
    "\n",
    "if len(filenames) == 1:\n",
    "    train_path_ids = filenames\n",
    "    test_path_ids = filenames\n",
    "else:\n",
    "    divider = int(len(filenames) * 0.9)\n",
    "    train_path_ids = filenames[:divider]\n",
    "    test_path_ids = filenames[divider:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp = False\n",
    "train_gen = DataGenerator(train_path_ids, clamp=clamp)\n",
    "test_gen = DataGenerator(test_path_ids, clamp=clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(train_gen[0][0][0], train_gen[0][0][0])\n",
    "assert len(train_gen[0][0]) == 3\n",
    "for i in range(3):\n",
    "    get_set = train_gen[0][0][0]\n",
    "    if len(get_set.shape) != 3:\n",
    "        print(get_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 64\n",
    "\n",
    "    history_inp = Input((LearningPlayer_v1.TIMESTEPS, LearningPlayer_v1.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer_v1.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer_v1.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = GRU(GRU_DIM, name='gru')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden = Dense(32, activation='relu', name='hidden')(concat)\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(hidden)\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 36/300 [==>...........................] - ETA: 43s - loss: 4.1565 - mean_squared_error: 4.1565Reload\n",
      "106/300 [=========>....................] - ETA: 26s - loss: 1.6429 - mean_squared_error: 1.6429Reload\n",
      "186/300 [=================>............] - ETA: 14s - loss: 1.0395 - mean_squared_error: 1.0395Reload\n",
      "269/300 [=========================>....] - ETA: 3s - loss: 0.7803 - mean_squared_error: 0.7803Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.7197 - mean_squared_error: 0.7197Reload\n",
      "300/300 [==============================] - 42s 141ms/step - loss: 0.7179 - mean_squared_error: 0.7179 - val_loss: 0.1577 - val_mean_squared_error: 0.1833\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.18331, saving model to combined0.h5\n",
      "Epoch 2/50\n",
      " 50/300 [====>.........................] - ETA: 28s - loss: 0.1721 - mean_squared_error: 0.1721Reload\n",
      "131/300 [============>.................] - ETA: 19s - loss: 0.1617 - mean_squared_error: 0.1617Reload\n",
      "214/300 [====================>.........] - ETA: 10s - loss: 0.1559 - mean_squared_error: 0.1559Reload\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.1499 - mean_squared_error: 0.1499Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.1495 - mean_squared_error: 0.1495Reload\n",
      "300/300 [==============================] - 41s 135ms/step - loss: 0.1496 - mean_squared_error: 0.1496 - val_loss: 0.1350 - val_mean_squared_error: 0.1295\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.18331 to 0.12948, saving model to combined0.h5\n",
      "Epoch 3/50\n",
      " 24/300 [=>............................] - ETA: 33s - loss: 0.1384 - mean_squared_error: 0.1384Reload\n",
      " 75/300 [======>.......................] - ETA: 28s - loss: 0.1213 - mean_squared_error: 0.1213Reload\n",
      "156/300 [==============>...............] - ETA: 17s - loss: 0.1192 - mean_squared_error: 0.1192Reload\n",
      "237/300 [======================>.......] - ETA: 7s - loss: 0.1153 - mean_squared_error: 0.1153Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.1127 - mean_squared_error: 0.1127Reload\n",
      "300/300 [==============================] - 41s 135ms/step - loss: 0.1129 - mean_squared_error: 0.1129 - val_loss: 0.1369 - val_mean_squared_error: 0.1092\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.12948 to 0.10921, saving model to combined0.h5\n",
      "Epoch 4/50\n",
      " 17/300 [>.............................] - ETA: 35s - loss: 0.1060 - mean_squared_error: 0.1060Reload\n",
      "115/300 [==========>...................] - ETA: 21s - loss: 0.1044 - mean_squared_error: 0.1044Reload\n",
      "179/300 [================>.............] - ETA: 14s - loss: 0.1026 - mean_squared_error: 0.1026Reload\n",
      "260/300 [=========================>....] - ETA: 4s - loss: 0.1002 - mean_squared_error: 0.1002Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0990 - mean_squared_error: 0.0990Reload\n",
      "300/300 [==============================] - 40s 134ms/step - loss: 0.0989 - mean_squared_error: 0.0989 - val_loss: 0.0580 - val_mean_squared_error: 0.0954\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.10921 to 0.09540, saving model to combined0.h5\n",
      "Epoch 5/50\n",
      " 43/300 [===>..........................] - ETA: 28s - loss: 0.1037 - mean_squared_error: 0.1037Reload\n",
      "123/300 [===========>..................] - ETA: 20s - loss: 0.0949 - mean_squared_error: 0.0949Reload\n",
      "206/300 [===================>..........] - ETA: 10s - loss: 0.0954 - mean_squared_error: 0.0954Reload\n",
      "286/300 [===========================>..] - ETA: 1s - loss: 0.0923 - mean_squared_error: 0.0923Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0918 - mean_squared_error: 0.0918Reload\n",
      "300/300 [==============================] - 40s 134ms/step - loss: 0.0920 - mean_squared_error: 0.0920 - val_loss: 0.1110 - val_mean_squared_error: 0.0897\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.09540 to 0.08975, saving model to combined0.h5\n",
      "Epoch 6/50\n",
      " 69/300 [=====>........................] - ETA: 25s - loss: 0.0936 - mean_squared_error: 0.0936Reload\n",
      "150/300 [==============>...............] - ETA: 17s - loss: 0.0907 - mean_squared_error: 0.0907Reload\n",
      "231/300 [======================>.......] - ETA: 8s - loss: 0.0885 - mean_squared_error: 0.0885Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0886 - mean_squared_error: 0.0886Reload\n",
      "300/300 [==============================] - 40s 133ms/step - loss: 0.0885 - mean_squared_error: 0.0885 - val_loss: 0.1308 - val_mean_squared_error: 0.0875\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.08975 to 0.08754, saving model to combined0.h5\n",
      "Epoch 7/50\n",
      " 14/300 [>.............................] - ETA: 34s - loss: 0.0779 - mean_squared_error: 0.0779Reload\n",
      " 25/300 [=>............................] - ETA: 46s - loss: 0.0817 - mean_squared_error: 0.0817Reload\n",
      " 95/300 [========>.....................] - ETA: 27s - loss: 0.0807 - mean_squared_error: 0.0807Reload\n",
      "208/300 [===================>..........] - ETA: 11s - loss: 0.0839 - mean_squared_error: 0.0839Reload\n",
      "256/300 [========================>.....] - ETA: 5s - loss: 0.0836 - mean_squared_error: 0.0836Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0831 - mean_squared_error: 0.0831Reload\n",
      "300/300 [==============================] - 40s 135ms/step - loss: 0.0832 - mean_squared_error: 0.0832 - val_loss: 0.0937 - val_mean_squared_error: 0.0851\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.08754 to 0.08508, saving model to combined0.h5\n",
      "Epoch 8/50\n",
      " 37/300 [==>...........................] - ETA: 32s - loss: 0.0900 - mean_squared_error: 0.0900Reload\n",
      "118/300 [==========>...................] - ETA: 21s - loss: 0.0819 - mean_squared_error: 0.0819Reload\n",
      "199/300 [==================>...........] - ETA: 11s - loss: 0.0807 - mean_squared_error: 0.0807Reload\n",
      "280/300 [===========================>..] - ETA: 2s - loss: 0.0788 - mean_squared_error: 0.0788Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0790 - mean_squared_error: 0.0790Reload\n",
      "300/300 [==============================] - 41s 135ms/step - loss: 0.0791 - mean_squared_error: 0.0791 - val_loss: 0.0548 - val_mean_squared_error: 0.0818\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.08508 to 0.08184, saving model to combined0.h5\n",
      "Epoch 9/50\n",
      " 63/300 [=====>........................] - ETA: 25s - loss: 0.0751 - mean_squared_error: 0.0751Reload\n",
      "144/300 [=============>................] - ETA: 17s - loss: 0.0785 - mean_squared_error: 0.0785Reload\n",
      "224/300 [=====================>........] - ETA: 8s - loss: 0.0792 - mean_squared_error: 0.0792Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0798 - mean_squared_error: 0.0798Reload\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.0797 - mean_squared_error: 0.0797 - val_loss: 0.0721 - val_mean_squared_error: 0.0821\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.08184\n",
      "Epoch 10/50\n",
      "  5/300 [..............................] - ETA: 38s - loss: 0.0754 - mean_squared_error: 0.0754Reload\n",
      " 85/300 [=======>......................] - ETA: 26s - loss: 0.0777 - mean_squared_error: 0.0777Reload\n",
      "166/300 [===============>..............] - ETA: 15s - loss: 0.0783 - mean_squared_error: 0.0783Reload\n",
      "246/300 [=======================>......] - ETA: 6s - loss: 0.0773 - mean_squared_error: 0.0773Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0772 - mean_squared_error: 0.0772Reload\n",
      "300/300 [==============================] - 42s 139ms/step - loss: 0.0771 - mean_squared_error: 0.0771 - val_loss: 0.1062 - val_mean_squared_error: 0.0760\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.08184 to 0.07605, saving model to combined0.h5\n",
      "Epoch 11/50\n",
      " 32/300 [==>...........................] - ETA: 32s - loss: 0.0793 - mean_squared_error: 0.0793Reload\n",
      "110/300 [==========>...................] - ETA: 22s - loss: 0.0749 - mean_squared_error: 0.0749Reload\n",
      "191/300 [==================>...........] - ETA: 13s - loss: 0.0720 - mean_squared_error: 0.0720Reload\n",
      "272/300 [==========================>...] - ETA: 3s - loss: 0.0733 - mean_squared_error: 0.0733Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.0730Reload\n",
      "300/300 [==============================] - 40s 135ms/step - loss: 0.0729 - mean_squared_error: 0.0729 - val_loss: 0.0852 - val_mean_squared_error: 0.0749\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.07605 to 0.07491, saving model to combined0.h5\n",
      "Epoch 12/50\n",
      "  7/300 [..............................] - ETA: 31s - loss: 0.0688 - mean_squared_error: 0.0688Reload\n",
      " 52/300 [====>.........................] - ETA: 33s - loss: 0.0753 - mean_squared_error: 0.0753Reload\n",
      "132/300 [============>.................] - ETA: 20s - loss: 0.0749 - mean_squared_error: 0.0749Reload\n",
      "213/300 [====================>.........] - ETA: 10s - loss: 0.0731 - mean_squared_error: 0.0731Reload\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.0729 - mean_squared_error: 0.0729Reload\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0732 - mean_squared_error: 0.0732Reload\n",
      "300/300 [==============================] - 40s 135ms/step - loss: 0.0732 - mean_squared_error: 0.0732 - val_loss: 0.0484 - val_mean_squared_error: 0.0734\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 0.07491 to 0.07345, saving model to combined0.h5\n",
      "Epoch 13/50\n",
      " 75/300 [======>.......................] - ETA: 25s - loss: 0.0702 - mean_squared_error: 0.0702Reload\n",
      "155/300 [==============>...............] - ETA: 16s - loss: 0.0720 - mean_squared_error: 0.0720Reload\n",
      "238/300 [======================>.......] - ETA: 7s - loss: 0.0709 - mean_squared_error: 0.0709Reload\n",
      "243/300 [=======================>......] - ETA: 6s - loss: 0.0711 - mean_squared_error: 0.0711"
     ]
    }
   ],
   "source": [
    "def train_model(fname='model.h5'):\n",
    "    combined_net = create_model()\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=2),\n",
    "        ModelCheckpoint(fname, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    combined_net.fit_generator(train_gen,\n",
    "              epochs=50,\n",
    "                steps_per_epoch=300,\n",
    "            validation_steps=100,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=test_gen,\n",
    "              shuffle=True,\n",
    "                workers=1,\n",
    "                max_queue_size=50,\n",
    "                use_multiprocessing=False\n",
    "              )\n",
    "    return combined_net\n",
    "\n",
    "for i in range(num_train):\n",
    "    train_model('combined' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def split_model(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('gru').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('gru').output.shape[1], ), name='vector_history_inp')\n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden = best_model.get_layer('hidden')(concat)\n",
    "    output = best_model.get_layer('output')(hidden)\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_29_sim11_model0\n",
      "3_29_sim11_model1\n",
      "3_29_sim11_model2\n",
      "3_29_sim11_model3\n",
      "3_29_sim11_model4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "\n",
    "for i in range(num_train):\n",
    "    combined_file = 'combined' + str(i) + '.h5'\n",
    "    model_folder_name = data_folder + '_model' + str(i)\n",
    "    \n",
    "    model_folder_path = Path('../models/', model_folder_name)\n",
    "    delete_dir(model_folder_path)\n",
    "    model_folder_path.mkdir()\n",
    "    \n",
    "    split_model(combined_file, model_folder_path)\n",
    "    print(model_folder_name)\n",
    "    subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', 'gs://landlord_ai/models/' + model_folder_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
