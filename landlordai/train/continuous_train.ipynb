{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: landlord-ai in /opt/conda/lib/python3.7/site-packages (0.1.32)\n",
      "Collecting keras.preprocessing\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.2)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mlflow.tensorflow (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for mlflow.tensorflow\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import mlflow\n",
    "\n",
    "from landlordai.game.player import LearningPlayer\n",
    "from google.cloud.storage.client import Client\n",
    "from dateutil import parser\n",
    "import string\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/4_13_stream1’: File exists\n"
     ]
    }
   ],
   "source": [
    "# which continuous stream to use\n",
    "train_index = 2\n",
    "num_train_batches = 10\n",
    "lai_bucket = \"hseokho-lai\"\n",
    "gs_model_bucket = \"stream_models/\"\n",
    "local_models_dir = \"models/\"\n",
    "stream_bucket = \"4_13_stream1\"\n",
    "model_bucket = \"4_13_stream1_model1\"\n",
    "models_prefix = gs_model_bucket + model_bucket\n",
    "#stream_bucket = '4_11_actualq4'\n",
    "data_dir = '../data/'\n",
    "\n",
    "!mkdir {data_dir}{stream_bucket}\n",
    "\n",
    "def next_stream_data_index():\n",
    "    return len(list(Client().list_blobs(lai_bucket, prefix=stream_bucket)))\n",
    "\n",
    "def last_k_train_batches(k=num_train_batches):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=stream_bucket))\n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in tqdm(last_k_blobs):\n",
    "        destination_uri = '{}/{}'.format(data_dir, blob.name) \n",
    "        local_files.append(destination_uri)\n",
    "        if not os.path.exists(destination_uri):\n",
    "            blob.download_to_filename(destination_uri)\n",
    "        \n",
    "    return local_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reload_latest_dataset(debug=True):\n",
    "    all_history_matrices = []\n",
    "    all_move_vectors = []\n",
    "    all_hand_vectors = []\n",
    "    all_y = []\n",
    "    \n",
    "    last_k_blobs = last_k_train_batches()\n",
    "    if debug:\n",
    "        print(last_k_blobs)\n",
    "        \n",
    "    for local_zip in tqdm(last_k_blobs):\n",
    "        try:\n",
    "            with np.load(local_zip) as npzfile:\n",
    "                all_history_matrices.append(npzfile['history_matrices'])\n",
    "                all_move_vectors.append(npzfile['move_vectors'])\n",
    "                all_hand_vectors.append(npzfile['hand_vectors'])\n",
    "                all_y.append(npzfile['y'])\n",
    "        except:\n",
    "            Path(local_zip).unlink()\n",
    "            \n",
    "    all_history_matrices = np.concatenate(all_history_matrices)\n",
    "    all_move_vectors = np.vstack(all_move_vectors)\n",
    "    all_hand_vectors = np.vstack(all_hand_vectors)\n",
    "    all_y = np.hstack(all_y)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(all_history_matrices, all_move_vectors, all_hand_vectors, all_y, test_size=0.1, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 96\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden1 = Dense(128, activation='relu', name='hidden1')(concat)\n",
    "    hidden2 = Dense(96, activation='relu', name='hidden2')(BatchNormalization(name='bn1')(hidden1))\n",
    "    hidden3 = Dense(64, activation='relu', name='hidden3')(BatchNormalization(name='bn2')(hidden2))\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn3')(hidden3))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net\n",
    "\n",
    "def strip_parent_folder(filename):\n",
    "    return '/'.join(filename.split('/')[1:])\n",
    "\n",
    "def random_from_last_k_models(k):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=models_prefix))\n",
    "    if len(all_blobs) == 0:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    # get k most recent blobs\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "    \n",
    "    # get their parents\n",
    "    recent_parents = [Path(x.name).parent for x in last_k_blobs]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in all_blobs:\n",
    "        blob_parent = Path(blob.name).parent\n",
    "        # use any blob that has matching parent\n",
    "        if blob_parent in recent_parents:\n",
    "            local_path = Path(local_models_dir) / strip_parent_folder(blob.name)\n",
    "            local_path.parent.parent.mkdir(exist_ok=True)\n",
    "            local_path.parent.mkdir(exist_ok=True)\n",
    "            local_files.append(local_path)\n",
    "            if not local_path.exists():\n",
    "                print(local_path)\n",
    "                blob.download_to_filename(str(local_path))\n",
    "        \n",
    "    return random.choice(list(set([f.parent for f in local_files])))\n",
    "\n",
    "def get_next_model_index():\n",
    "    # 3 is the number of files per model\n",
    "    return int(len(list(Client().list_blobs(lai_bucket, prefix=models_prefix))) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sanity_check_model(combined_file, net_dir):\n",
    "    num_samples = 1000\n",
    "    sanity_set = ((train_hm[:num_samples], train_mv[:num_samples], train_hv[:num_samples]), train_y[:num_samples])\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2\n",
    "    \n",
    "def split_model_triage(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    split_model(best_model, model_folder)\n",
    "    \n",
    "def split_model(best_model, model_folder):\n",
    "    bn1 = best_model.get_layer('bn1')\n",
    "    bn2 = best_model.get_layer('bn2')\n",
    "    bn3 = best_model.get_layer('bn3')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden1 = best_model.get_layer('hidden1')(concat)\n",
    "    hidden2 = best_model.get_layer('hidden2')(bn1(hidden1))\n",
    "    hidden3 = best_model.get_layer('hidden3')(bn2(hidden2))\n",
    "    output = best_model.get_layer('output')(bn3(hidden3))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))\n",
    "    best_model.save(str(model_folder / 'combined.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(p_create=0.1):\n",
    "    combined_model_file = random_from_last_k_models(10).absolute() / \"combined.h5\"\n",
    "    print(combined_model_file)\n",
    "    if combined_model_file:\n",
    "        combined_model = keras.models.load_model(combined_model_file)\n",
    "    if not combined_model_file or random.random() < p_create:\n",
    "        combined_model = create_model()\n",
    "        \n",
    "    local_model_hash = \"Z\" + ''.join(random.choices(string.ascii_letters + string.digits, k=16)) + '.h5'\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=3),\n",
    "        ModelCheckpoint(local_model_hash, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # refresh data    \n",
    "    train_hm, test_hm, train_mv, test_mv, train_hv, test_hv, train_y, test_y = reload_latest_dataset()\n",
    "    \n",
    "    combined_model.fit(x=[train_hm, train_mv, train_hv], y=train_y,\n",
    "                     batch_size=1 << 11,\n",
    "                epochs=5,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=([test_hm, test_mv, test_hv], test_y),\n",
    "                shuffle=True\n",
    "              )\n",
    "    return local_model_hash\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "\n",
    "def publish_model(combined_model_file):\n",
    "    model_folder_name = model_bucket + '_' + str(get_next_model_index())\n",
    "\n",
    "    model_folder_path = Path('../models/', model_folder_name)\n",
    "    delete_dir(model_folder_path)\n",
    "    model_folder_path.mkdir()\n",
    "\n",
    "    split_model_triage(combined_model_file, model_folder_path)\n",
    "    sanity_check_model(combined_model_file, model_folder_path)\n",
    "    print(model_folder_name)\n",
    "    #bucket = Client().get_bucket(lai_bucket)\n",
    "    #bucket.blob(gs_model_bucket + '/' + model_folder_name + '/' + ).upload_from_filename(combined_model_file)\n",
    "    subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', \"gs://\" + lai_bucket + '/' + gs_model_bucket + model_folder_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/landlord_ai/landlordai/train/models/4_13_stream1_model1_70/combined.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py:811: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if isinstance(loss, collections.Mapping):\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:348: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(values, collections.Sequence):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9cf5ff2a114c2e9ad6b118ca4d1c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['../data//4_13_stream1/102.npz', '../data//4_13_stream1/103.npz', '../data//4_13_stream1/104.npz', '../data//4_13_stream1/105.npz', '../data//4_13_stream1/106.npz', '../data//4_13_stream1/107.npz', '../data//4_13_stream1/108.npz', '../data//4_13_stream1/109.npz', '../data//4_13_stream1/110.npz', '../data//4_13_stream1/111.npz']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1405ed4436524519ba4656e8b06f1b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 2181334 samples, validate on 242371 samples\n",
      "Epoch 1/5\n",
      "2181334/2181334 [==============================] - 209s 96us/step - loss: 0.2003 - mean_squared_error: 0.2003 - val_loss: 0.2031 - val_mean_squared_error: 0.2031\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.20307, saving model to ZTuS21lGBSpEfKcMq.h5\n",
      "Epoch 2/5\n",
      "2181334/2181334 [==============================] - 208s 95us/step - loss: 0.1944 - mean_squared_error: 0.1944 - val_loss: 0.2088 - val_mean_squared_error: 0.2088\n",
      "\n",
      "Epoch 00002: val_mean_squared_error did not improve from 0.20307\n",
      "Epoch 3/5\n",
      "2181334/2181334 [==============================] - 208s 95us/step - loss: 0.1903 - mean_squared_error: 0.1903 - val_loss: 0.1966 - val_mean_squared_error: 0.1966\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.20307 to 0.19658, saving model to ZTuS21lGBSpEfKcMq.h5\n",
      "Epoch 4/5\n",
      " 309248/2181334 [===>..........................] - ETA: 2:52 - loss: 0.1802 - mean_squared_error: 0.1802"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    combined_file = train_model()\n",
    "    publish_model(combined_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
