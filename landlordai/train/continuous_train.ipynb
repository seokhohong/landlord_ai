{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.37.tar.gz (13 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.37-py3-none-any.whl size=17369 sha256=42b0cc89e69d64de55c10f5549c70ecf057b2f5e9cd6ce10d3efb9e026d52726\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f6/bb/7c/d992c0a8538255908a57e9ede6209a4749edc8a3d262d691cb\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "  Attempting uninstall: landlord-ai\n",
      "    Found existing installation: landlord-ai 0.1.35\n",
      "    Uninstalling landlord-ai-0.1.35:\n",
      "      Successfully uninstalled landlord-ai-0.1.35\n",
      "Successfully installed landlord-ai-0.1.37\n",
      "Collecting keras.preprocessing\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n",
      "Requirement already satisfied: mlflow in /opt/conda/lib/python3.7/site-packages (1.7.2)\n",
      "Requirement already satisfied: alembic<=1.4.1 in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.23.0)\n",
      "Requirement already satisfied: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.18.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.3.0)\n",
      "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /opt/conda/lib/python3.7/site-packages (from mlflow) (20.0.4)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.14.0)\n",
      "Requirement already satisfied: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.1.1)\n",
      "Requirement already satisfied: sqlalchemy<=1.3.13 in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.3.13)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow) (5.3.1)\n",
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.17.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.11.4)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (4.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (7.1.1)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.13.0)\n",
      "Requirement already satisfied: gorilla in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3.0)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.0.3)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow) (1.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2019.11.28)\n",
      "Requirement already satisfied: configparser>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow) (5.0.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.7/site-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (46.1.3.post20200325)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.0.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (2.11.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow) (4.0.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow) (0.7.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->mlflow) (2019.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic<=1.4.1->mlflow) (1.1.1)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import mlflow\n",
    "\n",
    "from landlordai.game.player import LearningPlayer, LearningPlayer_v2\n",
    "from google.cloud.storage.client import Client\n",
    "from dateutil import parser\n",
    "import string\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which continuous stream to use\n",
    "train_index = 2\n",
    "num_train_batches = 10\n",
    "lai_bucket = \"hseokho-lai\"\n",
    "gs_model_bucket = \"stream_models/\"\n",
    "local_models_dir = \"models/\"\n",
    "stream_bucket = \"4_13_stream2\"\n",
    "model_bucket = \"4_13_stream2_model1\"\n",
    "models_prefix = gs_model_bucket + model_bucket\n",
    "data_dir = '../data/'\n",
    "\n",
    "!mkdir {data_dir}{stream_bucket}\n",
    "\n",
    "def next_stream_data_index():\n",
    "    return len(list(Client().list_blobs(lai_bucket, prefix=stream_bucket)))\n",
    "\n",
    "def last_k_train_batches(k=num_train_batches):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=stream_bucket))\n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in tqdm(last_k_blobs):\n",
    "        destination_uri = '{}/{}'.format(data_dir, blob.name) \n",
    "        local_files.append(destination_uri)\n",
    "        if not os.path.exists(destination_uri):\n",
    "            blob.download_to_filename(destination_uri)\n",
    "        \n",
    "    return local_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reload_latest_dataset(debug=True, k=num_train_batches):\n",
    "    all_history_matrices = []\n",
    "    all_move_vectors = []\n",
    "    all_hand_vectors = []\n",
    "    all_y = []\n",
    "    \n",
    "    last_k_blobs = last_k_train_batches(k)\n",
    "    if debug:\n",
    "        print(last_k_blobs)\n",
    "        \n",
    "    for local_zip in tqdm(last_k_blobs):\n",
    "        try:\n",
    "            with np.load(local_zip) as npzfile:\n",
    "                all_history_matrices.append(npzfile['history_matrices'])\n",
    "                all_move_vectors.append(npzfile['move_vectors'])\n",
    "                all_hand_vectors.append(npzfile['hand_vectors'])\n",
    "                all_y.append(npzfile['y'])\n",
    "        except:\n",
    "            Path(local_zip).unlink()\n",
    "            \n",
    "    all_history_matrices = np.concatenate(all_history_matrices)\n",
    "    all_move_vectors = np.vstack(all_move_vectors)\n",
    "    all_hand_vectors = np.vstack(all_hand_vectors)\n",
    "    all_y = np.hstack(all_y)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(all_history_matrices, all_move_vectors, all_hand_vectors, all_y, test_size=0.1, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 160\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden1 = Dense(384, activation='relu', name='hidden1')(concat)\n",
    "    hidden2 = Dense(128, activation='relu', name='hidden2')(BatchNormalization(name='bn1')(hidden1))\n",
    "    hidden3 = Dense(64, activation='relu', name='hidden3')(BatchNormalization(name='bn2')(hidden2))\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn3')(hidden3))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net\n",
    "\n",
    "def strip_parent_folder(filename):\n",
    "    return '/'.join(filename.split('/')[1:])\n",
    "\n",
    "def random_from_last_k_models(k):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=models_prefix))\n",
    "    if len(all_blobs) == 0:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    # get k most recent blobs\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "    \n",
    "    # get their parents\n",
    "    recent_parents = [Path(x.name).parent for x in last_k_blobs]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in all_blobs:\n",
    "        blob_parent = Path(blob.name).parent\n",
    "        # use any blob that has matching parent\n",
    "        if blob_parent in recent_parents:\n",
    "            local_path = Path(local_models_dir) / strip_parent_folder(blob.name)\n",
    "            local_path.parent.parent.mkdir(exist_ok=True)\n",
    "            local_path.parent.mkdir(exist_ok=True)\n",
    "            local_files.append(local_path)\n",
    "            if not local_path.exists():\n",
    "                print(local_path)\n",
    "                blob.download_to_filename(str(local_path))\n",
    "        \n",
    "    return random.choice(list(set([f.parent for f in local_files])))\n",
    "\n",
    "def get_next_model_index():\n",
    "    # 3 is the number of files per model\n",
    "    return int(len(list(Client().list_blobs(lai_bucket, prefix=models_prefix))) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sanity_check_model(combined_file, net_dir):\n",
    "    num_samples = 1000\n",
    "    train_hm, test_hm, train_mv, test_mv, train_hv, test_hv, train_y, test_y = reload_latest_dataset(debug=False, k=1)\n",
    "    sanity_set = ((train_hm[:num_samples], train_mv[:num_samples], train_hv[:num_samples]), train_y[:num_samples])\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer_v2(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2\n",
    "    \n",
    "def split_model_triage(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    split_model(best_model, model_folder)\n",
    "    \n",
    "def split_model(best_model, model_folder):\n",
    "    bn1 = best_model.get_layer('bn1')\n",
    "    bn2 = best_model.get_layer('bn2')\n",
    "    bn3 = best_model.get_layer('bn3')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden1 = best_model.get_layer('hidden1')(concat)\n",
    "    hidden2 = best_model.get_layer('hidden2')(bn1(hidden1))\n",
    "    hidden3 = best_model.get_layer('hidden3')(bn2(hidden2))\n",
    "    output = best_model.get_layer('output')(bn3(hidden3))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))\n",
    "    best_model.save(str(model_folder / 'combined.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(p_create=0.1):\n",
    "    combined_model_file = random_from_last_k_models(10)\n",
    "    if combined_model_file:\n",
    "        combined_model_file = combined_model_file.absolute() / \"combined.h5\"\n",
    "        print(\"Resuming training from\", combined_model_file)\n",
    "        combined_model = keras.models.load_model(combined_model_file)\n",
    "    if not combined_model_file or random.random() < p_create:\n",
    "        print(\"Fresh Model!\")\n",
    "        combined_model = create_model()\n",
    "        \n",
    "    local_model_hash = \"Z\" + ''.join(random.choices(string.ascii_letters + string.digits, k=16)) + '.h5'\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=3),\n",
    "        ModelCheckpoint(local_model_hash, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # refresh data    \n",
    "    train_hm, test_hm, train_mv, test_mv, train_hv, test_hv, train_y, test_y = reload_latest_dataset()\n",
    "    \n",
    "    combined_model.fit(x=[train_hm, train_mv, train_hv], y=train_y,\n",
    "                     batch_size=1 << 11,\n",
    "                epochs=5,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=([test_hm, test_mv, test_hv], test_y),\n",
    "                shuffle=True\n",
    "              )\n",
    "    return local_model_hash\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "\n",
    "def publish_model(combined_model_file):\n",
    "    model_folder_name = model_bucket + '_' + str(get_next_model_index())\n",
    "\n",
    "    model_folder_path = Path('../models/', model_folder_name)\n",
    "    delete_dir(model_folder_path)\n",
    "    model_folder_path.mkdir()\n",
    "\n",
    "    split_model_triage(combined_model_file, model_folder_path)\n",
    "    sanity_check_model(combined_model_file, model_folder_path)\n",
    "    print(model_folder_name)\n",
    "    #bucket = Client().get_bucket(lai_bucket)\n",
    "    #bucket.blob(gs_model_bucket + '/' + model_folder_name + '/' + ).upload_from_filename(combined_model_file)\n",
    "    subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', \"gs://\" + lai_bucket + '/' + gs_model_bucket + model_folder_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh Model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61bf1bf649447a6a92aed7e01342c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    combined_file = train_model()\n",
    "    publish_model(combined_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
