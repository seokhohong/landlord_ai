{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting landlord-ai\n",
      "  Downloading landlord_ai-0.1.34.tar.gz (13 kB)\n",
      "Building wheels for collected packages: landlord-ai\n",
      "  Building wheel for landlord-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for landlord-ai: filename=landlord_ai-0.1.34-py3-none-any.whl size=17230 sha256=5a21e2882d68d3c3824270c3f6011b97ab90cf12b64b2b0757f1e1b77213ee2a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/1d/f3/4c/e6443e175cd18417392b45c75a3160f45bb9ef017eacd8c5e7\n",
      "Successfully built landlord-ai\n",
      "Installing collected packages: landlord-ai\n",
      "  Attempting uninstall: landlord-ai\n",
      "    Found existing installation: landlord-ai 0.1.32\n",
      "    Uninstalling landlord-ai-0.1.32:\n",
      "      Successfully uninstalled landlord-ai-0.1.32\n",
      "Successfully installed landlord-ai-0.1.34\n",
      "Collecting keras.preprocessing\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras.preprocessing) (1.18.2)\n",
      "Installing collected packages: keras.preprocessing\n",
      "Successfully installed keras.preprocessing\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.7.2-py3-none-any.whl (16.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.0 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (7.1.1)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.11.4)\n",
      "Collecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4.tar.gz (5.5 kB)\n",
      "Requirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.23.0)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3.1)\n",
      "Requirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (4.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.0.3)\n",
      "Collecting gorilla\n",
      "  Downloading gorilla-0.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.18.2)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.13.0.tar.gz (18 kB)\n",
      "Collecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 89.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow) (5.3.1)\n",
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.17.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.3.0)\n",
      "Collecting gunicorn; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 9.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.8.1)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.10.0.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy<=1.3.13\n",
      "  Downloading SQLAlchemy-1.3.13.tar.gz (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 82.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow) (4.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.0->mlflow) (46.1.3.post20200325)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->mlflow) (2019.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.0.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (2.11.1)\n",
      "Requirement already satisfied: prometheus_client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow) (0.7.1)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.2-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting configparser>=0.3.5\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
      "Building wheels for collected packages: querystring-parser, prometheus-flask-exporter, alembic, databricks-cli, sqlalchemy\n",
      "  Building wheel for querystring-parser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-py3-none-any.whl size=7076 sha256=91d2fc1c182677c2448cf9303275d7856f39e5ffe7c35f9e753cd4e88651f0a9\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/69/38/7a/072b5863ca334d012821a287fd1d066cea33abdcda3ef2f878\n",
      "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.13.0-py3-none-any.whl size=14944 sha256=9b922c3284767e79d546efe6687eb1eeeb4ee851f9403cb8fb50b1603e63d20b\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/2e/ba/02/e222fb4f349a6d1f99c1f5ccaaa7cd0be66e8206a60992400b\n",
      "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158154 sha256=46b5ed5f4d1d7de740d030bba8e37f8163815cd8af89949a2a3869a154fb47eb\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.10.0-py3-none-any.whl size=84284 sha256=d6c71e873c8daca16520fc14b35ea4e26007aea3eef8514f2e24cd9581e7bdf4\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/18/99/c4/a78238ec2573319736f680362bb2c4accd5f6cfe25ff2a1470\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.13-cp37-cp37m-linux_x86_64.whl size=1221889 sha256=afdf0c972b4e2ac3738e2d041988e8e4248e1b29f58371e6c6e3ff6881e7b195\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/b9/ba/77/163f10f14bd489351530603e750c195b0ceceed2f3be2b32f1\n",
      "Successfully built querystring-parser prometheus-flask-exporter alembic databricks-cli sqlalchemy\n",
      "Installing collected packages: querystring-parser, gorilla, prometheus-flask-exporter, sqlalchemy, Mako, python-editor, alembic, gunicorn, tabulate, configparser, databricks-cli, mlflow\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.3.15\n",
      "    Uninstalling SQLAlchemy-1.3.15:\n",
      "      Successfully uninstalled SQLAlchemy-1.3.15\n",
      "Successfully installed Mako-1.1.2 alembic-1.4.1 configparser-5.0.0 databricks-cli-0.10.0 gorilla-0.3.0 gunicorn-20.0.4 mlflow-1.7.2 prometheus-flask-exporter-0.13.0 python-editor-1.0.4 querystring-parser-1.2.4 sqlalchemy-1.3.13 tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "!pip install landlord-ai --upgrade\n",
    "!pip install keras.preprocessing --user\n",
    "!pip install tqdm\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import *\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import mlflow\n",
    "\n",
    "from landlordai.game.player import LearningPlayer\n",
    "from google.cloud.storage.client import Client\n",
    "from dateutil import parser\n",
    "import string\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/4_13_stream1’: File exists\n"
     ]
    }
   ],
   "source": [
    "# which continuous stream to use\n",
    "train_index = 2\n",
    "num_train_batches = 10\n",
    "lai_bucket = \"hseokho-lai\"\n",
    "gs_model_bucket = \"stream_models/\"\n",
    "local_models_dir = \"models/\"\n",
    "stream_bucket = \"4_13_stream1\"\n",
    "model_bucket = \"4_13_stream1_model1\"\n",
    "models_prefix = gs_model_bucket + model_bucket\n",
    "#stream_bucket = '4_11_actualq4'\n",
    "data_dir = '../data/'\n",
    "\n",
    "!mkdir {data_dir}{stream_bucket}\n",
    "\n",
    "def next_stream_data_index():\n",
    "    return len(list(Client().list_blobs(lai_bucket, prefix=stream_bucket)))\n",
    "\n",
    "def last_k_train_batches(k=num_train_batches):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=stream_bucket))\n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in tqdm(last_k_blobs):\n",
    "        destination_uri = '{}/{}'.format(data_dir, blob.name) \n",
    "        local_files.append(destination_uri)\n",
    "        if not os.path.exists(destination_uri):\n",
    "            blob.download_to_filename(destination_uri)\n",
    "        \n",
    "    return local_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reload_latest_dataset(debug=True, k=num_train_batches):\n",
    "    all_history_matrices = []\n",
    "    all_move_vectors = []\n",
    "    all_hand_vectors = []\n",
    "    all_y = []\n",
    "    \n",
    "    last_k_blobs = last_k_train_batches(k)\n",
    "    if debug:\n",
    "        print(last_k_blobs)\n",
    "        \n",
    "    for local_zip in tqdm(last_k_blobs):\n",
    "        try:\n",
    "            with np.load(local_zip) as npzfile:\n",
    "                all_history_matrices.append(npzfile['history_matrices'])\n",
    "                all_move_vectors.append(npzfile['move_vectors'])\n",
    "                all_hand_vectors.append(npzfile['hand_vectors'])\n",
    "                all_y.append(npzfile['y'])\n",
    "        except:\n",
    "            Path(local_zip).unlink()\n",
    "            \n",
    "    all_history_matrices = np.concatenate(all_history_matrices)\n",
    "    all_move_vectors = np.vstack(all_move_vectors)\n",
    "    all_hand_vectors = np.vstack(all_hand_vectors)\n",
    "    all_y = np.hstack(all_y)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(all_history_matrices, all_move_vectors, all_hand_vectors, all_y, test_size=0.1, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    K.clear_session()\n",
    "    GRU_DIM = 128\n",
    "\n",
    "    history_inp = Input((None, LearningPlayer.TIMESTEP_FEATURES), name='history_inp')\n",
    "    move_inp = Input((LearningPlayer.TIMESTEP_FEATURES, ), name='move_inp')\n",
    "    hand_inp = Input((LearningPlayer.HAND_FEATURES, ), name='hand_inp')\n",
    "    gru = Bidirectional(GRU(GRU_DIM, name='gru'), name='bidi')(history_inp)\n",
    "\n",
    "    concat = Concatenate()([gru, move_inp, hand_inp])\n",
    "    hidden1 = Dense(256, activation='relu', name='hidden1')(concat)\n",
    "    hidden2 = Dense(128, activation='relu', name='hidden2')(BatchNormalization(name='bn1')(hidden1))\n",
    "    hidden3 = Dense(64, activation='relu', name='hidden3')(BatchNormalization(name='bn2')(hidden2))\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(BatchNormalization(name='bn3')(hidden3))\n",
    "    combined_net = keras.models.Model(inputs=[history_inp, move_inp, hand_inp], outputs=output)\n",
    "    combined_net.compile(loss=keras.losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "    return combined_net\n",
    "\n",
    "def strip_parent_folder(filename):\n",
    "    return '/'.join(filename.split('/')[1:])\n",
    "\n",
    "def random_from_last_k_models(k):\n",
    "    num_batches = next_stream_data_index()\n",
    "    all_blobs = list(Client().list_blobs(lai_bucket, prefix=models_prefix))\n",
    "    if len(all_blobs) == 0:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    update_times = sorted([parser.parse(blob._properties['updated']) for blob in all_blobs], reverse=True)\n",
    "\n",
    "    top_update_times = update_times[:min(k, len(update_times))]\n",
    "\n",
    "    # get k most recent blobs\n",
    "    last_k_blobs = [blob for blob in all_blobs if parser.parse(blob._properties['updated']) in top_update_times]\n",
    "    \n",
    "    # get their parents\n",
    "    recent_parents = [Path(x.name).parent for x in last_k_blobs]\n",
    "\n",
    "    local_files = []\n",
    "    for blob in all_blobs:\n",
    "        blob_parent = Path(blob.name).parent\n",
    "        # use any blob that has matching parent\n",
    "        if blob_parent in recent_parents:\n",
    "            local_path = Path(local_models_dir) / strip_parent_folder(blob.name)\n",
    "            local_path.parent.parent.mkdir(exist_ok=True)\n",
    "            local_path.parent.mkdir(exist_ok=True)\n",
    "            local_files.append(local_path)\n",
    "            if not local_path.exists():\n",
    "                print(local_path)\n",
    "                blob.download_to_filename(str(local_path))\n",
    "        \n",
    "    return random.choice(list(set([f.parent for f in local_files])))\n",
    "\n",
    "def get_next_model_index():\n",
    "    # 3 is the number of files per model\n",
    "    return int(len(list(Client().list_blobs(lai_bucket, prefix=models_prefix))) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def has_layer(model, layer):\n",
    "    try:\n",
    "        model.get_layer(layer)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sanity_check_model(combined_file, net_dir):\n",
    "    num_samples = 1000\n",
    "    train_hm, test_hm, train_mv, test_mv, train_hv, test_hv, train_y, test_y = reload_latest_dataset(debug=False, k=1)\n",
    "    sanity_set = ((train_hm[:num_samples], train_mv[:num_samples], train_hv[:num_samples]), train_y[:num_samples])\n",
    "    historical_features, move_vectors, hand_vectors = sanity_set[0]\n",
    "    targets = sanity_set[1]\n",
    "\n",
    "    player = LearningPlayer(name='sanity', net_dir=str(net_dir))\n",
    "    \n",
    "    historical_matrix = player.history_net.predict(historical_features, batch_size=1024)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    error_1 = metrics.mean_squared_error(targets, player.get_position_predictions(historical_matrix, move_vectors, hand_vectors))\n",
    "    \n",
    "    composite = keras.models.load_model(combined_file)\n",
    "    error_2 = metrics.mean_squared_error(targets, composite.predict([historical_features, move_vectors, hand_vectors], batch_size=1024))\n",
    "    print(combined_file, error_1, error_2)\n",
    "    assert np.abs(error_1 - error_2) < 1E-2\n",
    "    \n",
    "def split_model_triage(composite, model_folder):\n",
    "    best_model = keras.models.load_model(composite)\n",
    "    \n",
    "    split_model(best_model, model_folder)\n",
    "    \n",
    "def split_model(best_model, model_folder):\n",
    "    bn1 = best_model.get_layer('bn1')\n",
    "    bn2 = best_model.get_layer('bn2')\n",
    "    bn3 = best_model.get_layer('bn3')\n",
    "    history_net = keras.models.Model(inputs=[best_model.get_layer('history_inp').input], outputs=[best_model.get_layer('bidi').output])\n",
    "\n",
    "    vector_history_inp = Input((best_model.get_layer('bidi').output.shape[1], ), name='vector_history_inp')\n",
    "    \n",
    "    concat = Concatenate()([vector_history_inp, best_model.get_layer('move_inp').output, best_model.get_layer('hand_inp').output])\n",
    "    hidden1 = best_model.get_layer('hidden1')(concat)\n",
    "    hidden2 = best_model.get_layer('hidden2')(bn1(hidden1))\n",
    "    hidden3 = best_model.get_layer('hidden3')(bn2(hidden2))\n",
    "    output = best_model.get_layer('output')(bn3(hidden3))\n",
    "\n",
    "    move_inp = best_model.get_layer('move_inp').input\n",
    "    hand_inp = best_model.get_layer('hand_inp').input\n",
    "    position_net = keras.models.Model(inputs=[vector_history_inp, move_inp, hand_inp], outputs=[output])\n",
    "\n",
    "    history_net.save(str(model_folder / 'history.h5'))\n",
    "    position_net.save(str(model_folder / 'position.h5'))\n",
    "    best_model.save(str(model_folder / 'combined.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(p_create=0.1):\n",
    "    combined_model_file = random_from_last_k_models(10).absolute() / \"combined.h5\"\n",
    "    print(combined_model_file)\n",
    "    if combined_model_file:\n",
    "        combined_model = keras.models.load_model(combined_model_file)\n",
    "    if not combined_model_file or random.random() < p_create:\n",
    "        combined_model = create_model()\n",
    "        \n",
    "    local_model_hash = \"Z\" + ''.join(random.choices(string.ascii_letters + string.digits, k=16)) + '.h5'\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=3),\n",
    "        ModelCheckpoint(local_model_hash, monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # refresh data    \n",
    "    train_hm, test_hm, train_mv, test_mv, train_hv, test_hv, train_y, test_y = reload_latest_dataset()\n",
    "    \n",
    "    combined_model.fit(x=[train_hm, train_mv, train_hv], y=train_y,\n",
    "                     batch_size=1 << 11,\n",
    "                epochs=5,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=([test_hm, test_mv, test_hv], test_y),\n",
    "                shuffle=True\n",
    "              )\n",
    "    return local_model_hash\n",
    "\n",
    "def delete_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    for file in path.iterdir():\n",
    "        os.remove(file)\n",
    "    path.rmdir()\n",
    "\n",
    "def publish_model(combined_model_file):\n",
    "    model_folder_name = model_bucket + '_' + str(get_next_model_index())\n",
    "\n",
    "    model_folder_path = Path('../models/', model_folder_name)\n",
    "    delete_dir(model_folder_path)\n",
    "    model_folder_path.mkdir()\n",
    "\n",
    "    split_model_triage(combined_model_file, model_folder_path)\n",
    "    sanity_check_model(combined_model_file, model_folder_path)\n",
    "    print(model_folder_name)\n",
    "    #bucket = Client().get_bucket(lai_bucket)\n",
    "    #bucket.blob(gs_model_bucket + '/' + model_folder_name + '/' + ).upload_from_filename(combined_model_file)\n",
    "    subprocess.check_output(['gsutil', 'cp', '-r', '../models/' + model_folder_name + '/*', \"gs://\" + lai_bucket + '/' + gs_model_bucket + model_folder_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/4_13_stream1_model1_72/combined.h5\n",
      "models/4_13_stream1_model1_72/history.h5\n",
      "models/4_13_stream1_model1_72/position.h5\n",
      "models/4_13_stream1_model1_73/combined.h5\n",
      "models/4_13_stream1_model1_73/history.h5\n",
      "models/4_13_stream1_model1_73/position.h5\n",
      "/home/jupyter/landlord_ai/landlordai/train/models/4_13_stream1_model1_73/combined.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6977734133c4c588c72e53d278b1e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['../data//4_13_stream1/106.npz', '../data//4_13_stream1/107.npz', '../data//4_13_stream1/108.npz', '../data//4_13_stream1/109.npz', '../data//4_13_stream1/110.npz', '../data//4_13_stream1/111.npz', '../data//4_13_stream1/112.npz', '../data//4_13_stream1/113.npz', '../data//4_13_stream1/114.npz', '../data//4_13_stream1/88.npz']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b193092d4d004086a5add01a49509f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 3243862 samples, validate on 360430 samples\n",
      "Epoch 1/5\n",
      "3243862/3243862 [==============================] - 310s 96us/step - loss: 0.2170 - mean_squared_error: 0.2170 - val_loss: 0.3886 - val_mean_squared_error: 0.3886\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.38857, saving model to Ze6D0cuA9kgFPR3st.h5\n",
      "Epoch 2/5\n",
      "3243862/3243862 [==============================] - 311s 96us/step - loss: 0.2110 - mean_squared_error: 0.2110 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.38857 to 0.20346, saving model to Ze6D0cuA9kgFPR3st.h5\n",
      "Epoch 3/5\n",
      "3243862/3243862 [==============================] - 309s 95us/step - loss: 0.2033 - mean_squared_error: 0.2033 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 0.20346\n",
      "Epoch 4/5\n",
      "3243862/3243862 [==============================] - 309s 95us/step - loss: 0.1997 - mean_squared_error: 0.1997 - val_loss: 0.2208 - val_mean_squared_error: 0.2208\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.20346\n",
      "Epoch 5/5\n",
      "3243862/3243862 [==============================] - 310s 95us/step - loss: 0.1959 - mean_squared_error: 0.1959 - val_loss: 0.2104 - val_mean_squared_error: 0.2104\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.20346\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reload_latest_dataset() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-04ee50a9d303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcombined_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpublish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0e70b3630900>\u001b[0m in \u001b[0;36mpublish_model\u001b[0;34m(combined_model_file)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0msplit_model_triage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0msanity_check_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#bucket = Client().get_bucket(lai_bucket)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a1620b7f7ed9>\u001b[0m in \u001b[0;36msanity_check_model\u001b[0;34m(combined_file, net_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msanity_check_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_hm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreload_latest_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msanity_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mhistorical_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanity_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reload_latest_dataset() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    combined_file = train_model()\n",
    "    publish_model(combined_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
